{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESM breakdown \n",
    "## ESM functionalities\n",
    "ESM( ESM_1 ESM_1b, ESM_2): general protein language model   \n",
    "ESMFold: structure prediction based on language model   \n",
    "EMS_1V: speicalize language model on protein variants   \n",
    "EMS_1f: design protein sequence conditioning on protein backbone (similar to MPNN)   High level programming: combine language model and MCMC to design sequences with   high fitness   \n",
    "We will start on ESM first.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESM model \n",
    "1. based on Language BERT(Bidirectional Encoder Representations from Transformers)   model (Maksed language modelling based on left and right surrounding)     \n",
    "2. general architecture:     \n",
    "2.1 input representation token embedding + position embedding    \n",
    "2.2 Transformer encoding layer   \n",
    "output of a layer can be used as input for a new layer ( Number of layers)   \n",
    "within transformer encoder:   \n",
    "MultiHeadAttention   \n",
    "Residual connencton + LayerNorm   \n",
    "FeedForward NN   \n",
    "Residual Connection + LayerNorm   \n",
    "2.3 update embedding for each masked postions, use NN for prediction  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I. Protein token representation\n",
    "\n",
    "code: https://github.com/facebookresearch/esm/blob/main/esm/data.py#L14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import Parameter \n",
    "\n",
    "# we simplify ESM Alphabet class here\n",
    "# ESM has many flags for differnt models\n",
    "class Alphabet:\n",
    "    def __init__(self):\n",
    "        # Define tokens\n",
    "        self.tokens = [\"<pad>\", \"<cls>\", \"<eos>\", \"<unk>\", \"-\", \n",
    "                       \"A\", \"R\", \"N\", \"D\", \"C\", \"Q\", \"E\", \"G\", \n",
    "                       \"H\", \"I\", \"L\", \"K\", \"M\", \"F\", \"P\", \"S\", \n",
    "                       \"T\", \"W\", \"Y\", \"V\"]\n",
    "        \n",
    "        # Map tokens to indices\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.tokens)}\n",
    "        self.idx_to_token = {idx: token for idx, token in enumerate(self.tokens)}\n",
    "        \n",
    "        # Define special token indices\n",
    "        self.padding_idx = self.token_to_idx[\"<pad>\"]\n",
    "        self.cls_idx = self.token_to_idx[\"<cls>\"]\n",
    "        self.eos_idx = self.token_to_idx[\"<eos>\"]\n",
    "        self.unk_idx = self.token_to_idx[\"<unk>\"]\n",
    "\n",
    "    def encode(self, sequence):\n",
    "        \"\"\"Convert a sequence of amino acids to indices.\"\"\"\n",
    "        return [self.token_to_idx.get(token, self.unk_idx) for token in sequence]\n",
    "\n",
    "    def decode(self, indices):\n",
    "        \"\"\"Convert a list of indices back to a sequence of tokens.\"\"\"\n",
    "        return \"\".join([self.idx_to_token[idx] for idx in indices if idx != self.padding_idx])\n",
    "\n",
    "    def vocab_size(self):\n",
    "        \"\"\"Return the total number of tokens in the vocabulary.\"\"\"\n",
    "        return len(self.tokens)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([17, 19, 19, 17, 15])\n",
      "torch.Size([60, 32])\n"
     ]
    }
   ],
   "source": [
    "seq='MPPMLSGLLARLVKLLLGRHGSALHWRAAGAATVLLVIVLLAGSYLAVLAERGAPGAQLI'\n",
    "tokenize=Alphabet()\n",
    "token=torch.tensor(tokenize.encode(seq),dtype=torch.long)\n",
    "\n",
    "print(token[:5])\n",
    "tokenize.vocab_size()\n",
    "embed_tokens=nn.Embedding(tokenize.vocab_size(),32)\n",
    "x=embed_tokens(token)\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II, postional embedding\n",
    "There are three ways to do as far as I am aware\n",
    "1. sinusoidalPostionalEmbedding from orignal Transformer (https://github.com/facebookresearch/esm/blob/main/esm/modules.py#L260)\n",
    "2. Rotory embedding\n",
    "3. learnable embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified SinusoidaPositionalEmbedding\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class SinusoidalPositionEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, max_len: int = 5000):\n",
    "        \"\"\"\n",
    "        Sinusoidal positional embedding.\n",
    "        \n",
    "        Args:\n",
    "            embedding_dim (int): The size of each embedding vector.\n",
    "            max_len (int): The maximum sequence length.\n",
    "        \"\"\"\n",
    "        super(SinusoidalPositionEmbedding, self).__init__()\n",
    "        \n",
    "        # Precompute the positional encodings for efficiency\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # Create a matrix of shape (max_len, embedding_dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)  # Shape: (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * -(math.log(10000.0) / embedding_dim))\n",
    "        \n",
    "        pe = torch.zeros(max_len, embedding_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Apply sine to even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Apply cosine to odd indices\n",
    "        \n",
    "        self.register_buffer(\"pe\", pe)  # Register as a buffer so it won't be updated during training\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Add sinusoidal positional embeddings to the input tensor.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, embedding_dim).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Input tensor with positional embeddings added.\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        if seq_len > self.max_len:\n",
    "            raise ValueError(f\"Sequence length ({seq_len}) exceeds maximum length ({self.max_len}).\")\n",
    "        return x + self.pe[:seq_len].unsqueeze(0)  # Add positional embeddings (broadcasted over the batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learnable \n",
    "# padding handling\n",
    "import torch.nn.functional as F\n",
    "class LearnedPositionalEmbedding(nn.Embedding):\n",
    "    \"\"\"\n",
    "    This module learns positional embeddings up to a fixed maximum size.\n",
    "    Padding ids are ignored by either offsetting based on padding_idx\n",
    "    or by setting padding_idx to None and ensuring that the appropriate\n",
    "    position ids are passed to the forward function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int):\n",
    "        if padding_idx is not None:\n",
    "            num_embeddings_ = num_embeddings + padding_idx + 1 # convention to start with padding_idx \n",
    "        else:\n",
    "            num_embeddings_ = num_embeddings\n",
    "        super().__init__(num_embeddings_, embedding_dim, padding_idx)\n",
    "        self.max_positions = num_embeddings\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n",
    "        if input.size(1) > self.max_positions:\n",
    "            raise ValueError(\n",
    "                f\"Sequence length {input.size(1)} above maximum \"\n",
    "                f\" sequence length of {self.max_positions}\"\n",
    "            )\n",
    "        mask = input.ne(self.padding_idx).int() # mask for padding (if padding ->0)\n",
    "        positions = (torch.cumsum(mask, dim=1).type_as(mask) * mask).long() + self.padding_idx\n",
    "        return F.embedding(\n",
    "            positions,\n",
    "            self.weight,\n",
    "            self.padding_idx,\n",
    "            self.max_norm,\n",
    "            self.norm_type,\n",
    "            self.scale_grad_by_freq,\n",
    "            self.sparse,\n",
    "        )\n",
    "    # all the self.* is inherted from nn.Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.weight shape: torch.Size([11, 4])\n",
      "Input shape: torch.Size([2, 6])\n",
      "Output shape: torch.Size([2, 6, 4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Example parameters\n",
    "num_embeddings = 10  # Maximum sequence length\n",
    "embedding_dim = 4    # Embedding size\n",
    "padding_idx = 0      # Index for padding token\n",
    "batch_size = 2       # Number of samples in a batch\n",
    "seq_length = 6       # Length of each sequence\n",
    "\n",
    "# Create embedding layer\n",
    "embedding = LearnedPositionalEmbedding(num_embeddings, embedding_dim, padding_idx)\n",
    "\n",
    "# Input tensor: shape [batch_size, seq_length]\n",
    "input_tensor = torch.tensor([[0, 1, 2, 3, 4, 5], \n",
    "                             [0, 1, 1, 2, 3, 3]])  # Sample input with padding and positions\n",
    "\n",
    "# Forward pass\n",
    "output = embedding(input_tensor)\n",
    "\n",
    "# Shapes\n",
    "print(\"self.weight shape:\", embedding.weight.shape)  # [num_embeddings_ + padding_idx + 1, embedding_dim]\n",
    "print(\"Input shape:\", input_tensor.shape)            # [batch, seq_length]\n",
    "print(\"Output shape:\", output.shape)  # [batch, seq_length, embedding_dim]\n",
    "print(output[0,0,:])   # lookup table for padding is [0]*embed_dim   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
