{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESM breakdown \n",
    "## ESM functionalities\n",
    "ESM( ESM_1 ESM_1b, ESM_2): general protein language model   \n",
    "ESMFold: structure prediction based on language model   \n",
    "EMS_1V: speicalize language model on protein variants   \n",
    "EMS_1f: design protein sequence conditioning on protein backbone (similar to MPNN)   High level programming: combine language model and MCMC to design sequences with   high fitness   \n",
    "We will start on ESM first.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESM model \n",
    "1. based on Language BERT(Bidirectional Encoder Representations from Transformers)   model (Maksed language modelling based on left and right surrounding)     \n",
    "2. general architecture:     \n",
    "2.1 input representation token embedding + position embedding    \n",
    "2.2 Transformer encoding layer   \n",
    "output of a layer can be used as input for a new layer ( Number of layers)   \n",
    "within transformer encoder:   \n",
    "MultiHeadAttention   \n",
    "Residual connencton + LayerNorm   \n",
    "FeedForward NN   \n",
    "Residual Connection + LayerNorm   \n",
    "2.3 update embedding for each masked postions, use NN for prediction  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I. Protein token representation\n",
    "\n",
    "code: https://github.com/facebookresearch/esm/blob/main/esm/data.py#L14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import Parameter \n",
    "\n",
    "# we simplify ESM Alphabet class here\n",
    "# ESM has many flags for differnt models, more flexibility in data handling : see data.ipynb\n",
    "class Alphabet:\n",
    "    def __init__(self):\n",
    "        # Define tokens\n",
    "        self.tokens = [\"<pad>\", \"<cls>\", \"<eos>\", \"<unk>\", \"-\", \n",
    "                       \"A\", \"R\", \"N\", \"D\", \"C\", \"Q\", \"E\", \"G\", \n",
    "                       \"H\", \"I\", \"L\", \"K\", \"M\", \"F\", \"P\", \"S\", \n",
    "                       \"T\", \"W\", \"Y\", \"V\"]\n",
    "        \n",
    "        # Map tokens to indices\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.tokens)}\n",
    "        self.idx_to_token = {idx: token for idx, token in enumerate(self.tokens)}\n",
    "        \n",
    "        # Define special token indices\n",
    "        self.padding_idx = self.token_to_idx[\"<pad>\"]\n",
    "        self.cls_idx = self.token_to_idx[\"<cls>\"]\n",
    "        self.eos_idx = self.token_to_idx[\"<eos>\"]\n",
    "        self.unk_idx = self.token_to_idx[\"<unk>\"]\n",
    "\n",
    "    def encode(self, sequence):\n",
    "        \"\"\"Convert a sequence of amino acids to indices.\"\"\"\n",
    "        return [self.token_to_idx.get(token, self.unk_idx) for token in sequence]\n",
    "\n",
    "    def decode(self, indices):\n",
    "        \"\"\"Convert a list of indices back to a sequence of tokens.\"\"\"\n",
    "        return \"\".join([self.idx_to_token[idx] for idx in indices if idx != self.padding_idx])\n",
    "\n",
    "    def vocab_size(self):\n",
    "        \"\"\"Return the total number of tokens in the vocabulary.\"\"\"\n",
    "        return len(self.tokens)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([17, 19, 19, 17, 15])\n",
      "torch.Size([60, 32])\n"
     ]
    }
   ],
   "source": [
    "seq='MPPMLSGLLARLVKLLLGRHGSALHWRAAGAATVLLVIVLLAGSYLAVLAERGAPGAQLI'\n",
    "tokenize=Alphabet()\n",
    "token=torch.tensor(tokenize.encode(seq),dtype=torch.long)\n",
    "\n",
    "print(token[:5])\n",
    "tokenize.vocab_size()\n",
    "embed_tokens=nn.Embedding(tokenize.vocab_size(),32,padding_idx=tokenize.padding_idx) # padding embedding will be zero\n",
    "x=embed_tokens(token)\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II, postional embedding\n",
    "There are three ways to do as far as I am aware\n",
    "1. sinusoidalPostionalEmbedding from orignal Transformer (https://github.com/facebookresearch/esm/blob/main/esm/modules.py#L260)\n",
    "2. Rotary embedding (mulptiplicative instead of addjetive)\n",
    "3. learnable embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified SinusoidaPositionalEmbedding\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class SinusoidalPositionEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, max_len: int = 5000):\n",
    "        \"\"\"\n",
    "        Sinusoidal positional embedding.\n",
    "        \n",
    "        Args:\n",
    "            embedding_dim (int): The size of each embedding vector.\n",
    "            max_len (int): The maximum sequence length.\n",
    "        \"\"\"\n",
    "        super(SinusoidalPositionEmbedding, self).__init__()\n",
    "        \n",
    "        # Precompute the positional encodings for efficiency\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # Create a matrix of shape (max_len, embedding_dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)  # Shape: (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * -(math.log(10000.0) / embedding_dim)) # base freq\n",
    "        \n",
    "        pe = torch.zeros(max_len, embedding_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Apply sine to even indices \n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Apply cosine to odd indices\n",
    "        \n",
    "        self.register_buffer(\"pe\", pe)  # Register as a buffer so it won't be updated during training\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Add sinusoidal positional embeddings to the input tensor.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, embedding_dim).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Input tensor with positional embeddings added.\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        if seq_len > self.max_len:\n",
    "            raise ValueError(f\"Sequence length ({seq_len}) exceeds maximum length ({self.max_len}).\")\n",
    "        return x + self.pe[:seq_len].unsqueeze(0)  # Add positional embeddings (broadcasted over the batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learnable \n",
    "# padding handling\n",
    "import torch.nn.functional as F\n",
    "class LearnedPositionalEmbedding(nn.Embedding):\n",
    "    \"\"\"\n",
    "    This module learns positional embeddings up to a fixed maximum size.\n",
    "    Padding ids are ignored by either offsetting based on padding_idx\n",
    "    or by setting padding_idx to None and ensuring that the appropriate\n",
    "    position ids are passed to the forward function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int):\n",
    "        if padding_idx is not None:\n",
    "            num_embeddings_ = num_embeddings + padding_idx + 1 # convention to start with padding_idx \n",
    "        else:\n",
    "            num_embeddings_ = num_embeddings\n",
    "        super().__init__(num_embeddings_, embedding_dim, padding_idx)\n",
    "        self.max_positions = num_embeddings\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n",
    "        if input.size(1) > self.max_positions:\n",
    "            raise ValueError(\n",
    "                f\"Sequence length {input.size(1)} above maximum \"\n",
    "                f\" sequence length of {self.max_positions}\"\n",
    "            )\n",
    "        mask = input.ne(self.padding_idx).int() # mask for padding (if padding ->0)\n",
    "        positions = (torch.cumsum(mask, dim=1).type_as(mask) * mask).long() + self.padding_idx\n",
    "        return F.embedding(\n",
    "            positions,\n",
    "            self.weight,\n",
    "            self.padding_idx,\n",
    "            self.max_norm,\n",
    "            self.norm_type,\n",
    "            self.scale_grad_by_freq,\n",
    "            self.sparse,\n",
    "        )\n",
    "    # all the self.* is inherted from nn.Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.weight shape: torch.Size([11, 4])\n",
      "Input shape: torch.Size([2, 6])\n",
      "Output shape: torch.Size([2, 6, 4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "# main idea is to generat a trainable lookup matrix as a lookup table, assign embedding based on index lookup\n",
    "\n",
    "# Example parameters\n",
    "num_embeddings = 10  # Maximum sequence length\n",
    "embedding_dim = 4    # Embedding size\n",
    "padding_idx = 0      # Index for padding token\n",
    "batch_size = 2       # Number of samples in a batch\n",
    "seq_length = 6       # Length of each sequence\n",
    "\n",
    "# Create embedding layer\n",
    "embedding = LearnedPositionalEmbedding(num_embeddings, embedding_dim, padding_idx)\n",
    "\n",
    "# Input tensor: shape [batch_size, seq_length]\n",
    "input_tensor = torch.tensor([[0, 1, 2, 3, 4, 5], \n",
    "                             [0, 1, 1, 2, 3, 3]])  # Sample input with padding and positions\n",
    "\n",
    "# Forward pass\n",
    "output = embedding(input_tensor)\n",
    "\n",
    "# Shapes\n",
    "print(\"self.weight shape:\", embedding.weight.shape)  # [num_embeddings_ + padding_idx + 1, embedding_dim]\n",
    "print(\"Input shape:\", input_tensor.shape)            # [batch, seq_length]\n",
    "print(\"Output shape:\", output.shape)  # [batch, seq_length, embedding_dim]\n",
    "print(output[0,0,:])   # lookup table for padding is [0]*embed_dim   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotory Embedding\n",
    "reference: https://arxiv.org/abs/2104.09864  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rotory Embedding\n",
    "#  Code from https://github.com/facebookresearch/esm/blob/main/esm/rotary_embedding.py\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(x, cos, sin):\n",
    "    cos = cos[:, : x.shape[-2], :]   # x.shape[-2] real seq len \n",
    "    sin = sin[:, : x.shape[-2], :]\n",
    "\n",
    "    return (x * cos) + (rotate_half(x) * sin)\n",
    "\n",
    "class RotaryEmbedding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The rotary position embeddings from RoFormer_ (Su et. al).\n",
    "    A crucial insight from the method is that the query and keys are\n",
    "    transformed by rotation matrices which depend on the relative positions.\n",
    "    Other implementations are available in the Rotary Transformer repo_ and in\n",
    "    GPT-NeoX_, GPT-NeoX was an inspiration\n",
    "    .. _RoFormer: https://arxiv.org/abs/2104.09864\n",
    "    .. _repo: https://github.com/ZhuiyiTechnology/roformer\n",
    "    .. _GPT-NeoX: https://github.com/EleutherAI/gpt-neox\n",
    "    .. warning: Please note that this embedding is not registered on purpose, as it is transformative\n",
    "        (it does not create the embedding dimension) and will likely be picked up (imported) on a ad-hoc basis\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, *_, **__):\n",
    "        super().__init__()\n",
    "        # Generate and save the inverse frequency buffer (non trainable)\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))  # inv_freq dim = 1/2*dim\n",
    "        self.register_buffer(\"inv_freq\", inv_freq) # so that inv_freq is part of the mode but non-trainable\n",
    "\n",
    "        self._seq_len_cached = None  # cached seq_len so that the same len seqnece do not need be recalculated\n",
    "        self._cos_cached = None\n",
    "        self._sin_cached = None\n",
    "\n",
    "    def _update_cos_sin_tables(self, x, seq_dimension=1):\n",
    "        seq_len = x.shape[seq_dimension]\n",
    "\n",
    "        # Reset the tables if the sequence length has changed,\n",
    "        # or if we're on a new device (possibly due to tracing for instance)\n",
    "        if seq_len != self._seq_len_cached or self._cos_cached.device != x.device:\n",
    "            self._seq_len_cached = seq_len\n",
    "            t = torch.arange(x.shape[seq_dimension], device=x.device).type_as(self.inv_freq) # \n",
    "            freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)  # i index, j inv_freq 2D table\n",
    "            emb = torch.cat((freqs, freqs), dim=-1).to(x.device) # duplicate freqs \n",
    "\n",
    "            self._cos_cached = emb.cos()[None, :, :] # [1, len, emb_dim]\n",
    "            self._sin_cached = emb.sin()[None, :, :] # \n",
    "\n",
    "        return self._cos_cached, self._sin_cached\n",
    "\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        self._cos_cached, self._sin_cached = self._update_cos_sin_tables(k, seq_dimension=-2)\n",
    "\n",
    "        return (\n",
    "            apply_rotary_pos_emb(q, self._cos_cached, self._sin_cached),\n",
    "            apply_rotary_pos_emb(k, self._cos_cached, self._sin_cached),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 0.1000, 0.0100, 0.0010])\n",
      "torch.Size([1, 4, 8])\n",
      "tensor([[[ 0.1000,  0.2000,  0.3000,  0.4000,  0.5000,  0.6000,  0.7000,\n",
      "           0.8000],\n",
      "         [-0.6076,  0.8552,  1.0849,  1.1984,  1.4597,  1.4928,  1.5109,\n",
      "           1.6012],\n",
      "         [-2.6170,  1.3270,  1.8536,  1.9952,  0.6719,  2.5138,  2.3375,\n",
      "           2.4040],\n",
      "         [-2.8842,  1.5973,  2.6058,  2.7904, -2.5182,  3.6344,  3.1796,\n",
      "           3.2084]]])\n"
     ]
    }
   ],
   "source": [
    "# example of rotary embedding\n",
    "x = torch.tensor([\n",
    "    [[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8],  # Position 1\n",
    "     [0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6],  # Position 2\n",
    "     [1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4],  # Position 3\n",
    "     [2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2]]  # Position 4\n",
    "])\n",
    "#x = x.unsqueeze(0) # [B, L, emb_dim]\n",
    "\n",
    "rotary_test=RotaryEmbedding(dim=8)\n",
    "print(rotary_test.inv_freq)\n",
    "sin, cos =rotary_test._update_cos_sin_tables(x)\n",
    "print(sin.shape)\n",
    "\n",
    "updated_x=apply_rotary_pos_emb(x, sin, cos)\n",
    "print(updated_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III. Contact prediction\n",
    "paper: https://www.biorxiv.org/content/10.1101/622803v4.full.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using attention matrix to predict residue contact, used in supervised trainig \n",
    "# remove append and pre_append through masking \n",
    "# symmetrilize the matrix and substract background (apc function)\n",
    "# concatenate lays* head, and regression all values into one value\n",
    "# then use activation function to convert it probablity.\n",
    "from torch import nn\n",
    "from typing import Optional\n",
    "def symmetrize(x):\n",
    "    \"Make layer symmetric in final two dimensions, used for contact prediction.\"\n",
    "    return x + x.transpose(-1, -2)\n",
    "\n",
    "\n",
    "def apc(x):\n",
    "    \"Perform average product correct, used for contact prediction.\"\n",
    "    a1 = x.sum(-1, keepdims=True)\n",
    "    a2 = x.sum(-2, keepdims=True)\n",
    "    a12 = x.sum((-1, -2), keepdims=True)\n",
    "\n",
    "    avg = a1 * a2\n",
    "    avg.div_(a12)  # in-place to reduce memory\n",
    "    normalized = x - avg\n",
    "    return normalized\n",
    "\n",
    "class ContactPredictionHead(nn.Module):\n",
    "    \"\"\"Performs symmetrization, apc, and computes a logistic regression on the output features\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        prepend_bos: bool,\n",
    "        append_eos: bool,\n",
    "        bias=True,\n",
    "        eos_idx: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.prepend_bos = prepend_bos\n",
    "        self.append_eos = append_eos\n",
    "        if append_eos and eos_idx is None:\n",
    "            raise ValueError(\"Using an alphabet with eos token, but no eos token was passed in.\")\n",
    "        self.eos_idx = eos_idx\n",
    "        self.regression = nn.Linear(in_features, 1, bias)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, tokens, attentions):\n",
    "        # remove eos token attentions\n",
    "        if self.append_eos:\n",
    "            eos_mask = tokens.ne(self.eos_idx).to(attentions)\n",
    "            eos_mask = eos_mask.unsqueeze(1) * eos_mask.unsqueeze(2)\n",
    "            attentions = attentions * eos_mask[:, None, None, :, :]\n",
    "            attentions = attentions[..., :-1, :-1]\n",
    "        # remove cls token attentions\n",
    "        if self.prepend_bos:\n",
    "            attentions = attentions[..., 1:, 1:]\n",
    "        batch_size, layers, heads, seqlen, _ = attentions.size()\n",
    "        attentions = attentions.view(batch_size, layers * heads, seqlen, seqlen)\n",
    "\n",
    "        # features: B x C x T x T\n",
    "        attentions = attentions.to(\n",
    "            self.regression.weight.device\n",
    "        )  # attentions always float32, may need to convert to float16\n",
    "        attentions = apc(symmetrize(attentions))  # \n",
    "        attentions = attentions.permute(0, 2, 3, 1)\n",
    "        return self.activation(self.regression(attentions).squeeze(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert updated representaion to logits\n",
    "class RobertaLMHead(nn.Module):\n",
    "    \"\"\"Head for masked language modeling.\"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, output_dim, weight):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(embed_dim, embed_dim)\n",
    "        self.layer_norm = ESM1bLayerNorm(embed_dim)\n",
    "        self.weight = weight\n",
    "        self.bias = nn.Parameter(torch.zeros(output_dim))\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = self.dense(features)\n",
    "        x = gelu(x)\n",
    "        x = self.layer_norm(x)\n",
    "        # project back to size of vocabulary with bias\n",
    "        x = F.linear(x, self.weight) + self.bias\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overvall ESM2 architecuture (unbelivably simple)\n",
    "\n",
    "ESM1 ESM1b are all similar, differnece in details, such as positional embedding, scaling, LayerNormalization method    \n",
    "Overall bulding structure:  \n",
    "generate token_embedding with position embedding   \n",
    "input into loops of transformer layers, save intermeditae and attentio matrix if needed  \n",
    "laynorm(updated representation), use it for logits prediction for masked postions  \n",
    "use (attention matrix), use it for prediction for contact   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ESM2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int = 33,\n",
    "        embed_dim: int = 1280,\n",
    "        attention_heads: int = 20,\n",
    "        alphabet: Union[esm.data.Alphabet, str] = \"ESM-1b\",\n",
    "        token_dropout: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers   # layer\n",
    "        self.embed_dim = embed_dim    # embedding_dim\n",
    "        self.attention_heads = attention_heads  # numb of heads\n",
    "        if not isinstance(alphabet, esm.data.Alphabet):\n",
    "            alphabet = esm.data.Alphabet.from_architecture(alphabet)  # alphbet class for tokens\n",
    "        self.alphabet = alphabet\n",
    "        # vacabulary size (including append and pre-append)\n",
    "        self.alphabet_size = len(alphabet)\n",
    "        # idx for various non-content toekn including padding\n",
    "        self.padding_idx = alphabet.padding_idx\n",
    "        self.mask_idx = alphabet.mask_idx\n",
    "        self.cls_idx = alphabet.cls_idx\n",
    "        self.eos_idx = alphabet.eos_idx\n",
    "        self.prepend_bos = alphabet.prepend_bos\n",
    "        self.append_eos = alphabet.append_eos\n",
    "        self.token_dropout = token_dropout\n",
    "\n",
    "        # a useful way to call submodule to intialize model (can be integrated with flags to load differnt models, see ESM1)\n",
    "        self._init_submodules()\n",
    "\n",
    "    def _init_submodules(self):\n",
    "        self.embed_scale = 1\n",
    "        self.embed_tokens = nn.Embedding(\n",
    "            self.alphabet_size,\n",
    "            self.embed_dim,\n",
    "            padding_idx=self.padding_idx,\n",
    "        )   # generate Embedding x, a lookup table : [ alphabet_size, emb_dim] \n",
    "\n",
    "        # Transformer layer: using rotary positional embedding\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerLayer(\n",
    "                    self.embed_dim,\n",
    "                    4 * self.embed_dim,\n",
    "                    self.attention_heads,\n",
    "                    add_bias_kv=False,\n",
    "                    use_esm1b_layer_norm=True,\n",
    "                    use_rotary_embeddings=True,\n",
    "                )\n",
    "                for _ in range(self.num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # model for predicting contact\n",
    "        self.contact_head = ContactPredictionHead(\n",
    "            self.num_layers * self.attention_heads,\n",
    "            self.prepend_bos,\n",
    "            self.append_eos,\n",
    "            eos_idx=self.eos_idx,\n",
    "        )\n",
    "        self.emb_layer_norm_after = ESM1bLayerNorm(self.embed_dim)\n",
    "\n",
    "\n",
    "        self.lm_head = RobertaLMHead(\n",
    "            embed_dim=self.embed_dim,\n",
    "            output_dim=self.alphabet_size,\n",
    "            weight=self.embed_tokens.weight,  # initial weights in look up table, convert dim to alphabet size\n",
    "        )\n",
    "\n",
    "    def forward(self, tokens, repr_layers=[], need_head_weights=False, return_contacts=False):\n",
    "        if return_contacts:\n",
    "            need_head_weights = True\n",
    "\n",
    "        assert tokens.ndim == 2\n",
    "        padding_mask = tokens.eq(self.padding_idx)  # B, T\n",
    "\n",
    "        x = self.embed_scale * self.embed_tokens(tokens) # scaling \n",
    "\n",
    "        if self.token_dropout:\n",
    "            x.masked_fill_((tokens == self.mask_idx).unsqueeze(-1), 0.0) # make mask position to zero\n",
    "            # x: B x T x C\n",
    "            mask_ratio_train = 0.15 * 0.8  # general maksing ratio\n",
    "            src_lengths = (~padding_mask).sum(-1)   # all non-padding sites\n",
    "            mask_ratio_observed = (tokens == self.mask_idx).sum(-1).to(x.dtype) / src_lengths # calculate masking ratio [B, ]\n",
    "            x = x * (1 - mask_ratio_train) / (1 - mask_ratio_observed)[:, None, None] # scaling based on aimed and true masking ratio\n",
    "\n",
    "        if padding_mask is not None:\n",
    "            x = x * (1 - padding_mask.unsqueeze(-1).type_as(x)) # make padding sito to zero\n",
    "\n",
    "        repr_layers = set(repr_layers)  # transformer layers to save intermediate resutls (representaion, attention)\n",
    "        hidden_representations = {}\n",
    "        if 0 in repr_layers:\n",
    "            hidden_representations[0] = x\n",
    "\n",
    "        if need_head_weights:\n",
    "            attn_weights = []\n",
    "\n",
    "        # (B, T, E) => (T, B, E)\n",
    "        x = x.transpose(0, 1) # needed to make data compatiable with pytroch nn.Transformer\n",
    "\n",
    "        if not padding_mask.any():\n",
    "            padding_mask = None\n",
    "\n",
    "\n",
    "        # loop through layers of transformers, and save intermediate results if needed\n",
    "        for layer_idx, layer in enumerate(self.layers):\n",
    "            x, attn = layer(\n",
    "                x,\n",
    "                self_attn_padding_mask=padding_mask,\n",
    "                need_head_weights=need_head_weights,\n",
    "            )\n",
    "            if (layer_idx + 1) in repr_layers:\n",
    "                hidden_representations[layer_idx + 1] = x.transpose(0, 1)\n",
    "            if need_head_weights:\n",
    "                # (H, B, T, T) => (B, H, T, T)\n",
    "                attn_weights.append(attn.transpose(1, 0))\n",
    "\n",
    "        x = self.emb_layer_norm_after(x)\n",
    "        x = x.transpose(0, 1)  # (T, B, E) => (B, T, E)\n",
    "\n",
    "        # last hidden representation should have layer norm applied\n",
    "        if (layer_idx + 1) in repr_layers:\n",
    "            hidden_representations[layer_idx + 1] = x\n",
    "        x = self.lm_head(x)  # convert represnetaion into logits\n",
    "\n",
    "        result = {\"logits\": x, \"representations\": hidden_representations}\n",
    "        if need_head_weights:\n",
    "            # attentions: B x L x H x T x T\n",
    "            attentions = torch.stack(attn_weights, 1)\n",
    "            if padding_mask is not None:\n",
    "                attention_mask = 1 - padding_mask.type_as(attentions)\n",
    "                attention_mask = attention_mask.unsqueeze(1) * attention_mask.unsqueeze(2) # generate 2D padding mask\n",
    "                attentions = attentions * attention_mask[:, None, None, :, :] # apply mask\n",
    "            result[\"attentions\"] = attentions\n",
    "            if return_contacts:\n",
    "                contacts = self.contact_head(tokens, attentions) # predict contact map\n",
    "                result[\"contacts\"] = contacts\n",
    "\n",
    "        return result\n",
    "\n",
    "    def predict_contacts(self, tokens):\n",
    "        return self(tokens, return_contacts=True)[\"contacts\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSA_transformer\n",
    "ESM contains this Msa_transformer model where MSA is used\n",
    "The overall model architecture is the same, expect in the transformer layer, axial transformer is used for efficiency.  \n",
    "Axial transformer contains a row and column attention (same ae that in Alphafold2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
