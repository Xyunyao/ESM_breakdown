{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESM breakdown \n",
    "## ESM functionalities\n",
    "ESM( ESM_1 ESM_1b, ESM_2): general protein language model   \n",
    "ESMFold: structure prediction based on language model   \n",
    "EMS_1V: speicalize language model on protein variants   \n",
    "EMS_1f: design protein sequence conditioning on protein backbone (similar to MPNN)   High level programming: combine language model and MCMC to design sequences with   high fitness   \n",
    "We will start on ESM first.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESM model \n",
    "1. based on Language BERT(Bidirectional Encoder Representations from Transformers)   model (Maksed language modelling based on left and right surrounding)     \n",
    "2. general architecture:     \n",
    "2.1 input representation token embedding + position embedding    \n",
    "2.2 Transformer encoding layer   \n",
    "output of a layer can be used as input for a new layer ( Number of layers)   \n",
    "within transformer encoder:   \n",
    "MultiHeadAttention   \n",
    "Residual connencton + LayerNorm   \n",
    "FeedForward NN   \n",
    "Residual Connection + LayerNorm   \n",
    "2.3 update embedding for each masked postions, use NN for prediction  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I. Protein token representation\n",
    "\n",
    "code: https://github.com/facebookresearch/esm/blob/main/esm/data.py#L14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import Parameter \n",
    "\n",
    "# we simplify ESM Alphabet class here\n",
    "# ESM has many flags for differnt models\n",
    "class Alphabet:\n",
    "    def __init__(self):\n",
    "        # Define tokens\n",
    "        self.tokens = [\"<pad>\", \"<cls>\", \"<eos>\", \"<unk>\", \"-\", \n",
    "                       \"A\", \"R\", \"N\", \"D\", \"C\", \"Q\", \"E\", \"G\", \n",
    "                       \"H\", \"I\", \"L\", \"K\", \"M\", \"F\", \"P\", \"S\", \n",
    "                       \"T\", \"W\", \"Y\", \"V\"]\n",
    "        \n",
    "        # Map tokens to indices\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.tokens)}\n",
    "        self.idx_to_token = {idx: token for idx, token in enumerate(self.tokens)}\n",
    "        \n",
    "        # Define special token indices\n",
    "        self.padding_idx = self.token_to_idx[\"<pad>\"]\n",
    "        self.cls_idx = self.token_to_idx[\"<cls>\"]\n",
    "        self.eos_idx = self.token_to_idx[\"<eos>\"]\n",
    "        self.unk_idx = self.token_to_idx[\"<unk>\"]\n",
    "\n",
    "    def encode(self, sequence):\n",
    "        \"\"\"Convert a sequence of amino acids to indices.\"\"\"\n",
    "        return [self.token_to_idx.get(token, self.unk_idx) for token in sequence]\n",
    "\n",
    "    def decode(self, indices):\n",
    "        \"\"\"Convert a list of indices back to a sequence of tokens.\"\"\"\n",
    "        return \"\".join([self.idx_to_token[idx] for idx in indices if idx != self.padding_idx])\n",
    "\n",
    "    def vocab_size(self):\n",
    "        \"\"\"Return the total number of tokens in the vocabulary.\"\"\"\n",
    "        return len(self.tokens)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([17, 19, 19, 17, 15])\n",
      "torch.Size([60, 32])\n"
     ]
    }
   ],
   "source": [
    "seq='MPPMLSGLLARLVKLLLGRHGSALHWRAAGAATVLLVIVLLAGSYLAVLAERGAPGAQLI'\n",
    "tokenize=Alphabet()\n",
    "token=torch.tensor(tokenize.encode(seq),dtype=torch.long)\n",
    "\n",
    "print(token[:5])\n",
    "tokenize.vocab_size()\n",
    "embed_tokens=nn.Embedding(tokenize.vocab_size(),32)\n",
    "x=embed_tokens(token)\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II, postional embedding\n",
    "There are three ways to do as far as I am aware\n",
    "1. sinusoidalPostionalEmbedding from orignal Transformer (https://github.com/facebookresearch/esm/blob/main/esm/modules.py#L260)\n",
    "2. Rotary embedding (mulptiplicative instead of addjetive)\n",
    "3. learnable embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified SinusoidaPositionalEmbedding\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class SinusoidalPositionEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, max_len: int = 5000):\n",
    "        \"\"\"\n",
    "        Sinusoidal positional embedding.\n",
    "        \n",
    "        Args:\n",
    "            embedding_dim (int): The size of each embedding vector.\n",
    "            max_len (int): The maximum sequence length.\n",
    "        \"\"\"\n",
    "        super(SinusoidalPositionEmbedding, self).__init__()\n",
    "        \n",
    "        # Precompute the positional encodings for efficiency\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # Create a matrix of shape (max_len, embedding_dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)  # Shape: (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * -(math.log(10000.0) / embedding_dim)) # base freq\n",
    "        \n",
    "        pe = torch.zeros(max_len, embedding_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Apply sine to even indices \n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Apply cosine to odd indices\n",
    "        \n",
    "        self.register_buffer(\"pe\", pe)  # Register as a buffer so it won't be updated during training\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Add sinusoidal positional embeddings to the input tensor.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, embedding_dim).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Input tensor with positional embeddings added.\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        if seq_len > self.max_len:\n",
    "            raise ValueError(f\"Sequence length ({seq_len}) exceeds maximum length ({self.max_len}).\")\n",
    "        return x + self.pe[:seq_len].unsqueeze(0)  # Add positional embeddings (broadcasted over the batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learnable \n",
    "# padding handling\n",
    "import torch.nn.functional as F\n",
    "class LearnedPositionalEmbedding(nn.Embedding):\n",
    "    \"\"\"\n",
    "    This module learns positional embeddings up to a fixed maximum size.\n",
    "    Padding ids are ignored by either offsetting based on padding_idx\n",
    "    or by setting padding_idx to None and ensuring that the appropriate\n",
    "    position ids are passed to the forward function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int):\n",
    "        if padding_idx is not None:\n",
    "            num_embeddings_ = num_embeddings + padding_idx + 1 # convention to start with padding_idx \n",
    "        else:\n",
    "            num_embeddings_ = num_embeddings\n",
    "        super().__init__(num_embeddings_, embedding_dim, padding_idx)\n",
    "        self.max_positions = num_embeddings\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n",
    "        if input.size(1) > self.max_positions:\n",
    "            raise ValueError(\n",
    "                f\"Sequence length {input.size(1)} above maximum \"\n",
    "                f\" sequence length of {self.max_positions}\"\n",
    "            )\n",
    "        mask = input.ne(self.padding_idx).int() # mask for padding (if padding ->0)\n",
    "        positions = (torch.cumsum(mask, dim=1).type_as(mask) * mask).long() + self.padding_idx\n",
    "        return F.embedding(\n",
    "            positions,\n",
    "            self.weight,\n",
    "            self.padding_idx,\n",
    "            self.max_norm,\n",
    "            self.norm_type,\n",
    "            self.scale_grad_by_freq,\n",
    "            self.sparse,\n",
    "        )\n",
    "    # all the self.* is inherted from nn.Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.weight shape: torch.Size([11, 4])\n",
      "Input shape: torch.Size([2, 6])\n",
      "Output shape: torch.Size([2, 6, 4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "# main idea is to generat a trainable lookup matrix as a lookup table, assign embedding based on index lookup\n",
    "\n",
    "# Example parameters\n",
    "num_embeddings = 10  # Maximum sequence length\n",
    "embedding_dim = 4    # Embedding size\n",
    "padding_idx = 0      # Index for padding token\n",
    "batch_size = 2       # Number of samples in a batch\n",
    "seq_length = 6       # Length of each sequence\n",
    "\n",
    "# Create embedding layer\n",
    "embedding = LearnedPositionalEmbedding(num_embeddings, embedding_dim, padding_idx)\n",
    "\n",
    "# Input tensor: shape [batch_size, seq_length]\n",
    "input_tensor = torch.tensor([[0, 1, 2, 3, 4, 5], \n",
    "                             [0, 1, 1, 2, 3, 3]])  # Sample input with padding and positions\n",
    "\n",
    "# Forward pass\n",
    "output = embedding(input_tensor)\n",
    "\n",
    "# Shapes\n",
    "print(\"self.weight shape:\", embedding.weight.shape)  # [num_embeddings_ + padding_idx + 1, embedding_dim]\n",
    "print(\"Input shape:\", input_tensor.shape)            # [batch, seq_length]\n",
    "print(\"Output shape:\", output.shape)  # [batch, seq_length, embedding_dim]\n",
    "print(output[0,0,:])   # lookup table for padding is [0]*embed_dim   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotory Embedding\n",
    "reference: https://arxiv.org/abs/2104.09864  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rotory Embedding\n",
    "#  Code from https://github.com/facebookresearch/esm/blob/main/esm/rotary_embedding.py\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(x, cos, sin):\n",
    "    cos = cos[:, : x.shape[-2], :]   # x.shape[-2] real seq len \n",
    "    sin = sin[:, : x.shape[-2], :]\n",
    "\n",
    "    return (x * cos) + (rotate_half(x) * sin)\n",
    "\n",
    "class RotaryEmbedding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The rotary position embeddings from RoFormer_ (Su et. al).\n",
    "    A crucial insight from the method is that the query and keys are\n",
    "    transformed by rotation matrices which depend on the relative positions.\n",
    "    Other implementations are available in the Rotary Transformer repo_ and in\n",
    "    GPT-NeoX_, GPT-NeoX was an inspiration\n",
    "    .. _RoFormer: https://arxiv.org/abs/2104.09864\n",
    "    .. _repo: https://github.com/ZhuiyiTechnology/roformer\n",
    "    .. _GPT-NeoX: https://github.com/EleutherAI/gpt-neox\n",
    "    .. warning: Please note that this embedding is not registered on purpose, as it is transformative\n",
    "        (it does not create the embedding dimension) and will likely be picked up (imported) on a ad-hoc basis\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, *_, **__):\n",
    "        super().__init__()\n",
    "        # Generate and save the inverse frequency buffer (non trainable)\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))  # inv_freq dim = 1/2*dim\n",
    "        self.register_buffer(\"inv_freq\", inv_freq) # so that inv_freq is part of the mode but non-trainable\n",
    "\n",
    "        self._seq_len_cached = None  # cached seq_len so that the same len seqnece do not need be recalculated\n",
    "        self._cos_cached = None\n",
    "        self._sin_cached = None\n",
    "\n",
    "    def _update_cos_sin_tables(self, x, seq_dimension=1):\n",
    "        seq_len = x.shape[seq_dimension]\n",
    "\n",
    "        # Reset the tables if the sequence length has changed,\n",
    "        # or if we're on a new device (possibly due to tracing for instance)\n",
    "        if seq_len != self._seq_len_cached or self._cos_cached.device != x.device:\n",
    "            self._seq_len_cached = seq_len\n",
    "            t = torch.arange(x.shape[seq_dimension], device=x.device).type_as(self.inv_freq) # \n",
    "            freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)  # i index, j inv_freq 2D table\n",
    "            emb = torch.cat((freqs, freqs), dim=-1).to(x.device) # duplicate freqs \n",
    "\n",
    "            self._cos_cached = emb.cos()[None, :, :] # [1, len, emb_dim]\n",
    "            self._sin_cached = emb.sin()[None, :, :] # \n",
    "\n",
    "        return self._cos_cached, self._sin_cached\n",
    "\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        self._cos_cached, self._sin_cached = self._update_cos_sin_tables(k, seq_dimension=-2)\n",
    "\n",
    "        return (\n",
    "            apply_rotary_pos_emb(q, self._cos_cached, self._sin_cached),\n",
    "            apply_rotary_pos_emb(k, self._cos_cached, self._sin_cached),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 0.1000, 0.0100, 0.0010])\n",
      "torch.Size([1, 4, 8])\n",
      "tensor([[[ 0.1000,  0.2000,  0.3000,  0.4000,  0.5000,  0.6000,  0.7000,\n",
      "           0.8000],\n",
      "         [-0.6076,  0.8552,  1.0849,  1.1984,  1.4597,  1.4928,  1.5109,\n",
      "           1.6012],\n",
      "         [-2.6170,  1.3270,  1.8536,  1.9952,  0.6719,  2.5138,  2.3375,\n",
      "           2.4040],\n",
      "         [-2.8842,  1.5973,  2.6058,  2.7904, -2.5182,  3.6344,  3.1796,\n",
      "           3.2084]]])\n"
     ]
    }
   ],
   "source": [
    "# example of rotary embedding\n",
    "x = torch.tensor([\n",
    "    [[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8],  # Position 1\n",
    "     [0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6],  # Position 2\n",
    "     [1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4],  # Position 3\n",
    "     [2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2]]  # Position 4\n",
    "])\n",
    "#x = x.unsqueeze(0) # [B, L, emb_dim]\n",
    "\n",
    "rotary_test=RotaryEmbedding(dim=8)\n",
    "print(rotary_test.inv_freq)\n",
    "sin, cos =rotary_test._update_cos_sin_tables(x)\n",
    "print(sin.shape)\n",
    "\n",
    "updated_x=apply_rotary_pos_emb(x, sin, cos)\n",
    "print(updated_x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
