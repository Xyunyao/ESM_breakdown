{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakdown overview\n",
    "1. subject: ESM_inverse_fold (https://github.com/facebookresearch/esm/tree/main/esm/inverse_folding)\n",
    "2. aim of ESM_inverse_fold: conditon on backbone structure to recover sequence (software with similar functionality: ProteinMPNN)  \n",
    "3. Overall architecture:  \n",
    "Geometric GVP-GNN as initial layers, followed by\n",
    "sequence-to-sequence Transformer encoder and decoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric GVP_GNN layer (representation layer)\n",
    "1. feature.py (take cooridnates to caluclate features (VECOTR OR SCALER))    \n",
    "1.1 _diheadral (omega, psi, phi/ cos/sin  scaler)  \n",
    "1.2 _orientation (CA-CA(i-1/i+1),  Vector)   \n",
    "1.3 _sidechain  (calcualte CA-CB vector based on CA N CO, vector)  \n",
    "1.4 _dist (calculate pair distance ), output k-neasrt neighbour ( dist (B,l,k),   E_idx (B,L,K), cooridnate_mask (B,L,K, whether the distance is calculable,such as   masked), residue_mask(B,L,K, whehter it is padding))      \n",
    "1.5 _position_embedding: sinusoidual postion embedding  \n",
    "1.6 get_node_feature: combine 1.1 and 1.2 1.3   \n",
    "1.7 get_edge_feature: basically combine 1.4 and 1.5 (details are bit complex and   illustrated later) \n",
    "\n",
    "1.8 GVPGraphEmbedding: call 1.6 and 1.7 to get feature, use a GVP layer to embed these features \n",
    "\n",
    "2. gvp_module.py\n",
    "2.1 basic model to handle mixture of vector scaler info and update embedding by passing info betwen vector and scaler   \n",
    "vector's norm is concat to scaler to pass info  \n",
    "scaler is used as gating to pass info  \n",
    "![GVP](GVP_cartoon.png)\n",
    "\n",
    "2.2 Message passing with GVP (GVP-GNN)\n",
    "![GVP_GNN_1](GVP_GNN_1.png)\n",
    "![GVP_GNN_2](GVP_GNN_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion of Invariant and equvarient for translation and rotation\n",
    "1. translation\n",
    "It is obvious that the model is invariant to translation as inter-atom vector and torsion angle were used for input. \n",
    "2. rotation \n",
    "GVP is only equivariant to ratation, i.e.  GVP(R.X)=R.GVP(X)\n",
    "as Linear(Linear(R(x1, x2, x3)))= R(Linear(Linear(x1, x2, x3))), here Linear is a elementwise operation on indivually x1, x2, x3\n",
    "GVP-GNN is invariant to ratation, as the GNN contain rotation invariant part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Encoder and Decoder layer\n",
    "\n",
    "### Encoder layer:\n",
    "x <- LayerNorm(x)  \n",
    "x <- SelfAttention(x)    \n",
    "x <- Dropout(x)    \n",
    "x <- Residual_connect(x)  \n",
    "x <- LayerNorm(x)    \n",
    "x <- MLP(ReLu(MLP(x)))  \n",
    "x <- Dropout(x)   \n",
    "x <- Residual_connect(x)  \n",
    "\n",
    "### Decoder layer:  \n",
    "\n",
    "x <- LayerNorm(x)   \n",
    "x <- SelfAttention(x) , with cached K V to avoid recomputate (explain: https://medium.com/@joaolages/kv-caching-explained-276520203249)     \n",
    "x <- Dropout(x)    \n",
    "x <- Residual_connect(x)  \n",
    "x <- LayerNorm(x)    \n",
    "x, attn <- Encoder Attention (q=x, key=encoder_output, v=encoder_output), cached KV    \n",
    "x <- Dropout(x)   \n",
    "x <- Residual_connect(x)  \n",
    "x <- LayerNorm(x)   \n",
    "x <- ReLu(MLP(x))  \n",
    "x <- LayerNorm(x)   \n",
    "x <- Dropout(MLP(x))   \n",
    "x <- scaling(x) with learnable scaling factors  \n",
    "x <- Residual_connect(x)  \n",
    "output: x, attn  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build GVPTransfomer_encoder\n",
    "\n",
    "the major part is to generate embedding x for transformer input\n",
    "It contains:\n",
    "1. initial features \n",
    "self.embed_gvp_input_features = nn.Linear(15, embed_dim)\n",
    "scalar_features, vector_features = GVPInputFeaturizer.get_node_features(\n",
    "            coords, coord_mask, with_coord_mask=False)\n",
    "        features = torch.cat([\n",
    "            scalar_features,\n",
    "            rotate(vector_features, R.transpose(-2, -1)).flatten(-2, -1),\n",
    "        ], dim=-1)\n",
    "components[\"gvp_input_features\"] = self.embed_gvp_input_features(features)\n",
    "2. confidence embedding: rbf (bin=16)\n",
    "self.embed_confidence = nn.Linear(16, embed_dim)\n",
    "3. Dihedral angle (it seems redundant with scaler part in 1)\n",
    "self.embed_dihedrals = DihedralFeatures(embed_dim)\n",
    "4. output from GVP encoder \n",
    "gvp_out_dim = gvp_args.node_hidden_dim_scalar + (3 *\n",
    "                gvp_args.node_hidden_dim_vector)\n",
    "self.embed_gvp_output = nn.Linear(gvp_out_dim, embed_dim)\n",
    "gvp_out_scalars, gvp_out_vectors = self.gvp_encoder(coords,\n",
    "                coord_mask, padding_mask, confidence)\n",
    "        R = get_rotation_frames(coords)\n",
    "        # Rotate to local rotation frame for rotation-invariance\n",
    "        gvp_out_features = torch.cat([\n",
    "            gvp_out_scalars,\n",
    "            rotate(gvp_out_vectors, R.transpose(-2, -1)).flatten(-2, -1),\n",
    "        ], dim=-1)\n",
    "components[\"gvp_out\"] = self.embed_gvp_output(gvp_out_features)\n",
    "5. mask token embedding \n",
    "\n",
    "mask_tokens = (\n",
    "    padding_mask * self.dictionary.padding_idx + \n",
    "    ~padding_mask * self.dictionary.get_idx(\"<mask>\")\n",
    ")\n",
    "components[\"tokens\"] = self.embed_tokens(mask_tokens) * self.embed_scale\n",
    "\n",
    "\n",
    "6. sinusoidual position embedding\n",
    "\n",
    "x is sum over 1 to 6:\n",
    "embed = sum(components.values())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Transformer Decoder\n",
    "\n",
    "###input x:\n",
    "\n",
    "x = self.embed_scale * self.embed_tokens(prev_output_tokens)\n",
    "\n",
    "if self.project_in_dim is not None:\n",
    "    x = self.project_in_dim(x)\n",
    "\n",
    "x += positions\n",
    "\n",
    "###Decoder layer\n",
    "enc: output from above GVPTransformerEncoder \n",
    "\n",
    "for idx, layer in enumerate(self.layers):\n",
    "    if incremental_state is None:\n",
    "        self_attn_mask = self.buffered_future_mask(x)\n",
    "    else:\n",
    "        self_attn_mask = None\n",
    "\n",
    "    x, layer_attn, _ = layer(\n",
    "        x,\n",
    "        enc,\n",
    "        padding_mask,\n",
    "        incremental_state,\n",
    "        self_attn_mask=self_attn_mask,\n",
    "        self_attn_padding_mask=self_attn_padding_mask,\n",
    "        need_attn=False,\n",
    "        need_head_weights=False,\n",
    "    )\n",
    "    inner_states.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Sampling \n",
    "\n",
    "There are a few strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# greedy search\n",
    "def greedy_search_algorithm(input_ids, model, max_length):\n",
    "    for _ in range(max_length - input_ids.shape[1]):\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits[:, -1, :]  # Get logits of the last token\n",
    "        next_token = torch.argmax(logits, dim=-1)  # Select token with the highest probability\n",
    "        input_ids = torch.cat([input_ids, next_token.unsqueeze(-1)], dim=-1)\n",
    "        if next_token.item() == tokenizer.eos_token_id:  # Stop if EOS token is generated\n",
    "            break\n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "#multinomial sampling\n",
    "def multinomial_sampling_algorithm(input_ids, model, max_length, temperature=1.0):\n",
    "    for _ in range(max_length - input_ids.shape[1]):\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits[:, -1, :]  # Get logits of the last token\n",
    "        logits = logits / temperature  # Apply temperature scaling\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probabilities, num_samples=1)  # Sample from the distribution\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "        if next_token.item() == tokenizer.eos_token_id:  # Stop if EOS token is generated\n",
    "            break\n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "#beam search\n",
    "\n",
    "def beam_search_algorithm(input_ids, model, max_length, num_beams=5):\n",
    "    sequences = [[input_ids, 0]]  # List of (sequence, score)\n",
    "    for _ in range(max_length - input_ids.shape[1]):\n",
    "        all_candidates = []\n",
    "        for seq, score in sequences:\n",
    "            outputs = model(seq)\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            probabilities = torch.softmax(logits, dim=-1)\n",
    "            top_tokens = torch.topk(probabilities, num_beams, dim=-1)\n",
    "            for token, prob in zip(top_tokens.indices[0], top_tokens.values[0]):\n",
    "                candidate = [torch.cat([seq, token.unsqueeze(-1)], dim=-1), score - torch.log(prob)]\n",
    "                all_candidates.append(candidate)\n",
    "        sequences = sorted(all_candidates, key=lambda x: x[1])[:num_beams]  # Keep top beams\n",
    "        if all(seq[0][-1].item() == tokenizer.eos_token_id for seq in sequences):\n",
    "            break\n",
    "    return tokenizer.decode(sequences[0][0][0], skip_special_tokens=True)\n",
    "\n",
    "#beam search with multinomidal sampling\n",
    "\n",
    "def beam_search_multinomial_algorithm(input_ids, model, max_length, num_beams=5, top_k=50):\n",
    "    sequences = [[input_ids, 0]]  # List of (sequence, score)\n",
    "    for _ in range(max_length - input_ids.shape[1]):\n",
    "        all_candidates = []\n",
    "        for seq, score in sequences:\n",
    "            outputs = model(seq)\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            probabilities = torch.softmax(logits, dim=-1)\n",
    "            top_k_tokens = torch.topk(probabilities, top_k, dim=-1)\n",
    "            sampled_token = torch.multinomial(top_k_tokens.values[0], num_samples=1)\n",
    "            token = top_k_tokens.indices[0, sampled_token]\n",
    "            prob = top_k_tokens.values[0, sampled_token]\n",
    "            candidate = [torch.cat([seq, token.unsqueeze(-1)], dim=-1), score - torch.log(prob)]\n",
    "            all_candidates.append(candidate)\n",
    "        sequences = sorted(all_candidates, key=lambda x: x[1])[:num_beams]\n",
    "        if all(seq[0][-1].item() == tokenizer.eos_token_id for seq in sequences):\n",
    "            break\n",
    "    return tokenizer.decode(sequences[0][0][0], skip_special_tokens=True)\n",
    "\n",
    "#contrasive search\n",
    "def contrastive_search_algorithm(input_ids, model, max_length, top_k=5, penalty_alpha=0.6):\n",
    "    generated = input_ids\n",
    "    for _ in range(max_length - input_ids.shape[1]):\n",
    "        outputs = model(generated)\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        top_k_logits, top_k_indices = torch.topk(logits, top_k, dim=-1)\n",
    "        probabilities = torch.softmax(top_k_logits, dim=-1)\n",
    "        diversity_penalty = penalty_alpha * torch.log(probabilities)\n",
    "        adjusted_scores = probabilities - diversity_penalty\n",
    "        next_token = top_k_indices[0, torch.argmax(adjusted_scores)]\n",
    "        generated = torch.cat([generated, next_token.unsqueeze(-1)], dim=-1)\n",
    "        if next_token.item() == tokenizer.eos_token_id:  # Stop if EOS token is generated\n",
    "            break\n",
    "    return tokenizer.decode(generated[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampling in ESM_IF is multinomial sampling\n",
    "def sample(self, coords, partial_seq=None, temperature=1.0, confidence=None, device=None):\n",
    "        \"\"\"\n",
    "        Samples sequences based on multinomial sampling (no beam search).\n",
    "\n",
    "        Args:\n",
    "            coords: L x 3 x 3 list representing one backbone\n",
    "            partial_seq: Optional, partial sequence with mask tokens if part of\n",
    "                the sequence is known\n",
    "            temperature: sampling temperature, use low temperature for higher\n",
    "                sequence recovery and high temperature for higher diversity\n",
    "            confidence: optional length L list of confidence scores for coordinates\n",
    "        \"\"\"\n",
    "        L = len(coords)\n",
    "        # Convert to batch format\n",
    "        batch_converter = CoordBatchConverter(self.decoder.dictionary)\n",
    "        batch_coords, confidence, _, _, padding_mask = (\n",
    "            batch_converter([(coords, confidence, None)], device=device)\n",
    "        )\n",
    "        \n",
    "        # Start with prepend token\n",
    "        mask_idx = self.decoder.dictionary.get_idx('<mask>')\n",
    "        sampled_tokens = torch.full((1, 1+L), mask_idx, dtype=int)\n",
    "        sampled_tokens[0, 0] = self.decoder.dictionary.get_idx('<cath>')\n",
    "        if partial_seq is not None:\n",
    "            for i, c in enumerate(partial_seq):\n",
    "                sampled_tokens[0, i+1] = self.decoder.dictionary.get_idx(c)\n",
    "            \n",
    "        # Save incremental states for faster sampling\n",
    "        incremental_state = dict()\n",
    "        \n",
    "        # Run encoder only once\n",
    "        encoder_out = self.encoder(batch_coords, padding_mask, confidence)\n",
    "        \n",
    "        # Make sure all tensors are on the same device if a GPU is present\n",
    "        if device:\n",
    "            sampled_tokens = sampled_tokens.to(device)\n",
    "        \n",
    "        # Decode one token at a time\n",
    "        for i in range(1, L+1):\n",
    "            logits, _ = self.decoder(\n",
    "                sampled_tokens[:, :i], \n",
    "                encoder_out,\n",
    "                incremental_state=incremental_state,\n",
    "            )\n",
    "            logits = logits[0].transpose(0, 1)\n",
    "            logits /= temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            if sampled_tokens[0, i] == mask_idx:\n",
    "                sampled_tokens[:, i] = torch.multinomial(probs, 1).squeeze(-1)\n",
    "        sampled_seq = sampled_tokens[0, 1:]\n",
    "        \n",
    "        # Convert back to string via lookup\n",
    "        return ''.join([self.decoder.dictionary.get_tok(a) for a in sampled_seq])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
