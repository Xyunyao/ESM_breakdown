{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data handling in ESM\n",
    "\n",
    "We will study Alphabet class \n",
    "(code: https://github.com/facebookresearch/esm/blob/main/esm/data.py)\n",
    "1. vocabulary\n",
    "2. split text into tokens\n",
    "key: how to split text containing special tokens, not so important in PLM, but crticial in language model : \"Hello [CLS] world [SEP]!\"  \n",
    "3. batch processing \n",
    "    3.1 using MSA\n",
    "    3.2 dont use MSA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "from typing import Sequence, Tuple, List, Union\n",
    "import pickle\n",
    "import re\n",
    "import shutil\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "proteinseq_toks = {\n",
    "    'toks': ['L', 'A', 'G', 'V', 'S', 'E', 'R', 'T', 'I', 'D', 'P', 'K', 'Q', 'N', 'F', 'Y', 'M', 'H', 'W', 'C', 'X', 'B', 'U', 'Z', 'O', '.', '-']\n",
    "}\n",
    "\n",
    "class Alphabet(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        standard_toks: Sequence[str],\n",
    "        prepend_toks: Sequence[str] = (\"<null_0>\", \"<pad>\", \"<eos>\", \"<unk>\"),\n",
    "        append_toks: Sequence[str] = (\"<cls>\", \"<mask>\", \"<sep>\"),\n",
    "        prepend_bos: bool = True,\n",
    "        append_eos: bool = False,\n",
    "        use_msa: bool = False,\n",
    "    ):\n",
    "        self.standard_toks = list(standard_toks)\n",
    "        self.prepend_toks = list(prepend_toks)\n",
    "        self.append_toks = list(append_toks)\n",
    "        self.prepend_bos = prepend_bos\n",
    "        self.append_eos = append_eos\n",
    "        self.use_msa = use_msa\n",
    "\n",
    "        self.all_toks = list(self.prepend_toks)\n",
    "        self.all_toks.extend(self.standard_toks)\n",
    "        # question\n",
    "        for i in range((8 - (len(self.all_toks) % 8)) % 8):\n",
    "            self.all_toks.append(f\"<null_{i  + 1}>\")\n",
    "        self.all_toks.extend(self.append_toks)\n",
    "\n",
    "        # generae tok to idx dictionary\n",
    "        self.tok_to_idx = {tok: i for i, tok in enumerate(self.all_toks)}\n",
    "\n",
    "        self.unk_idx = self.tok_to_idx[\"<unk>\"]\n",
    "        self.padding_idx = self.get_idx(\"<pad>\")\n",
    "        self.cls_idx = self.get_idx(\"<cls>\")\n",
    "        self.mask_idx = self.get_idx(\"<mask>\")\n",
    "        self.eos_idx = self.get_idx(\"<eos>\")\n",
    "        self.all_special_tokens = ['<eos>', '<unk>', '<pad>', '<cls>', '<mask>']\n",
    "        self.unique_no_split_tokens = self.all_toks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_toks)\n",
    "\n",
    "    def get_idx(self, tok):\n",
    "        return self.tok_to_idx.get(tok, self.unk_idx)\n",
    "\n",
    "    def get_tok(self, ind):\n",
    "        return self.all_toks[ind]\n",
    "\n",
    "    def to_dict(self):\n",
    "        return self.tok_to_idx.copy()\n",
    "\n",
    "    def get_batch_converter(self, truncation_seq_length: int = None):\n",
    "        if self.use_msa:\n",
    "            return MSABatchConverter(self, truncation_seq_length)\n",
    "        else:\n",
    "            return BatchConverter(self, truncation_seq_length)\n",
    "\n",
    "    # load different vacabulary based on differnt models\n",
    "    @classmethod\n",
    "    def from_architecture(cls, name: str) -> \"Alphabet\":\n",
    "        if name in (\"ESM-1\", \"protein_bert_base\"):\n",
    "            standard_toks = proteinseq_toks[\"toks\"]\n",
    "            prepend_toks: Tuple[str, ...] = (\"<null_0>\", \"<pad>\", \"<eos>\", \"<unk>\")\n",
    "            append_toks: Tuple[str, ...] = (\"<cls>\", \"<mask>\", \"<sep>\")\n",
    "            prepend_bos = True\n",
    "            append_eos = False\n",
    "            use_msa = False\n",
    "        elif name in (\"ESM-1b\", \"roberta_large\"):\n",
    "            standard_toks = proteinseq_toks[\"toks\"]\n",
    "            prepend_toks = (\"<cls>\", \"<pad>\", \"<eos>\", \"<unk>\")\n",
    "            append_toks = (\"<mask>\",)\n",
    "            prepend_bos = True\n",
    "            append_eos = True\n",
    "            use_msa = False\n",
    "        elif name in (\"MSA Transformer\", \"msa_transformer\"):\n",
    "            standard_toks = proteinseq_toks[\"toks\"]\n",
    "            prepend_toks = (\"<cls>\", \"<pad>\", \"<eos>\", \"<unk>\")\n",
    "            append_toks = (\"<mask>\",)\n",
    "            prepend_bos = True\n",
    "            append_eos = False\n",
    "            use_msa = True\n",
    "        elif \"invariant_gvp\" in name.lower():\n",
    "            standard_toks = proteinseq_toks[\"toks\"]\n",
    "            prepend_toks = (\"<null_0>\", \"<pad>\", \"<eos>\", \"<unk>\")\n",
    "            append_toks = (\"<mask>\", \"<cath>\", \"<af2>\")\n",
    "            prepend_bos = True\n",
    "            append_eos = False\n",
    "            use_msa = False\n",
    "        else:\n",
    "            raise ValueError(\"Unknown architecture selected\")\n",
    "        return cls(standard_toks, prepend_toks, append_toks, prepend_bos, append_eos, use_msa)\n",
    "\n",
    "    def _tokeinize(self,text)-> str:\n",
    "        return text.split()\n",
    "\n",
    "    # complexity come from handling edge and special token \n",
    "\n",
    "    def tokenize(self, text, **kwargs) -> List[str]:\n",
    "        \"\"\"\n",
    "        Inspired by https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_utils.py\n",
    "        Converts a string in a sequence of tokens, using the tokenizer.\n",
    "\n",
    "        1,Identify special tokens that shouldn't be split.(in)\n",
    "        2.Process the text, splitting on special tokens.\n",
    "        3.Further tokenize non-special tokens using _tokenize.\n",
    "        4.Combine all tokens into a flat list and return.\n",
    "\n",
    "        Args:\n",
    "            text (:obj:`str`):\n",
    "                The sequence to be encoded.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`List[str]`: The list of tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        def split_on_token(tok, text):\n",
    "            result = []\n",
    "            split_text = text.split(tok)\n",
    "            for i, sub_text in enumerate(split_text):\n",
    "                # AddedToken can control whitespace stripping around them.\n",
    "                # We use them for GPT2 and Roberta to have different behavior depending on the special token\n",
    "                # Cf. https://github.com/huggingface/transformers/pull/2778\n",
    "                # and https://github.com/huggingface/transformers/issues/3788\n",
    "                # We strip left and right by default\n",
    "                if i < len(split_text) - 1:\n",
    "                    sub_text = sub_text.rstrip()\n",
    "                if i > 0:\n",
    "                    sub_text = sub_text.lstrip()\n",
    "\n",
    "                if i == 0 and not sub_text:\n",
    "                    result.append(tok)\n",
    "                elif i == len(split_text) - 1:\n",
    "                    if sub_text:\n",
    "                        result.append(sub_text)\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    if sub_text:\n",
    "                        result.append(sub_text)\n",
    "                    result.append(tok)\n",
    "            return result\n",
    "\n",
    "        def split_on_tokens(tok_list, text):\n",
    "            if not text.strip():\n",
    "                return []\n",
    "\n",
    "            tokenized_text = []\n",
    "            text_list = [text]\n",
    "            for tok in tok_list:\n",
    "                tokenized_text = []\n",
    "                for sub_text in text_list:\n",
    "                    if sub_text not in self.unique_no_split_tokens:\n",
    "                        tokenized_text.extend(split_on_token(tok, sub_text))\n",
    "                    else:\n",
    "                        tokenized_text.append(sub_text)\n",
    "                text_list = tokenized_text\n",
    "\n",
    "            return list(\n",
    "                itertools.chain.from_iterable(\n",
    "                    (\n",
    "                        self._tokenize(token)\n",
    "                        if token not in self.unique_no_split_tokens\n",
    "                        else [token]\n",
    "                        for token in tokenized_text\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "        no_split_token = self.unique_no_split_tokens\n",
    "        tokenized_text = split_on_tokens(no_split_token, text)\n",
    "        return tokenized_text\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.tok_to_idx[tok] for tok in self.tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchConverter(object):\n",
    "    \"\"\"Callable to convert an unprocessed (labels + strings) batch to a\n",
    "    processed (labels + tensor) batch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alphabet, truncation_seq_length: int = None):\n",
    "        self.alphabet = alphabet\n",
    "        self.truncation_seq_length = truncation_seq_length\n",
    "\n",
    "    def __call__(self, raw_batch: Sequence[Tuple[str, str]]):\n",
    "        # RoBERTa uses an eos token, while ESM-1 does not.\n",
    "        batch_size = len(raw_batch)\n",
    "        batch_labels, seq_str_list = zip(*raw_batch)\n",
    "        seq_encoded_list = [self.alphabet.encode(seq_str) for seq_str in seq_str_list]\n",
    "        if self.truncation_seq_length:\n",
    "            seq_encoded_list = [seq_str[:self.truncation_seq_length] for seq_str in seq_encoded_list]\n",
    "        max_len = max(len(seq_encoded) for seq_encoded in seq_encoded_list)\n",
    "        tokens = torch.empty(\n",
    "            (\n",
    "                batch_size,\n",
    "                max_len + int(self.alphabet.prepend_bos) + int(self.alphabet.append_eos),\n",
    "            ),\n",
    "            dtype=torch.int64,\n",
    "        )\n",
    "        tokens.fill_(self.alphabet.padding_idx)\n",
    "        labels = []\n",
    "        strs = []\n",
    "\n",
    "        for i, (label, seq_str, seq_encoded) in enumerate(\n",
    "            zip(batch_labels, seq_str_list, seq_encoded_list)\n",
    "        ):\n",
    "            labels.append(label)\n",
    "            strs.append(seq_str)\n",
    "            if self.alphabet.prepend_bos:\n",
    "                tokens[i, 0] = self.alphabet.cls_idx\n",
    "            seq = torch.tensor(seq_encoded, dtype=torch.int64)\n",
    "            tokens[\n",
    "                i,\n",
    "                int(self.alphabet.prepend_bos) : len(seq_encoded)\n",
    "                + int(self.alphabet.prepend_bos),\n",
    "            ] = seq\n",
    "            if self.alphabet.append_eos:\n",
    "                tokens[i, len(seq_encoded) + int(self.alphabet.prepend_bos)] = self.alphabet.eos_idx\n",
    "\n",
    "        return labels, strs, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Protein1', 'Protein2', 'Protein3', 'Protein4', 'Protein5']\n",
      "['MVLSPADKTNVKAAWGKVGAHAGEYGAEALERMFLSFPTTKTYFPHFD', 'GAGKLVWGKVNVDEVGGEALGRLLVVYPWTQRFFDSFGDLSNPGAVMGNPK', 'MGDVEKGKKIFVQKCAQCHTVEKGGKHKTGPNEKGQTAFQEALAAKKIL', 'MKLFWLLFTIGFCWAISAEGILHNFSLVRDSQSLEDLGDKLEDLRNL', 'MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPW']\n",
      "tensor([[ 0, 20,  7,  4,  8, 14,  5, 13, 15, 11, 17,  7, 15,  5,  5, 22,  6, 15,\n",
      "          7,  6,  5, 21,  5,  6,  9, 19,  6,  5,  9,  5,  4,  9, 10, 20, 18,  4,\n",
      "          8, 18, 14, 11, 11, 15, 11, 19, 18, 14, 21, 18, 13,  2,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1],\n",
      "        [ 0,  6,  5,  6, 15,  4,  7, 22,  6, 15,  7, 17,  7, 13,  9,  7,  6,  6,\n",
      "          9,  5,  4,  6, 10,  4,  4,  7,  7, 19, 14, 22, 11, 16, 10, 18, 18, 13,\n",
      "          8, 18,  6, 13,  4,  8, 17, 14,  6,  5,  7, 20,  6, 17, 14, 15,  2,  1,\n",
      "          1,  1,  1,  1,  1],\n",
      "        [ 0, 20,  6, 13,  7,  9, 15,  6, 15, 15, 12, 18,  7, 16, 15, 23,  5, 16,\n",
      "         23, 21, 11,  7,  9, 15,  6,  6, 15, 21, 15, 11,  6, 14, 17,  9, 15,  6,\n",
      "         16, 11,  5, 18, 16,  9,  5,  4,  5,  5, 15, 15, 12,  4,  2,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1],\n",
      "        [ 0, 20, 15,  4, 18, 22,  4,  4, 18, 11, 12,  6, 18, 23, 22,  5, 12,  8,\n",
      "          5,  9,  6, 12,  4, 21, 17, 18,  8,  4,  7, 10, 13,  8, 16,  8,  4,  9,\n",
      "         13,  4,  6, 13, 15,  4,  9, 13,  4, 10, 17,  4,  2,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1],\n",
      "        [ 0, 20,  8, 15,  6,  9,  9,  4, 18, 11,  6,  7,  7, 14, 12,  4,  7,  9,\n",
      "          4, 13,  6, 13,  7, 17,  6, 21, 15, 18,  8,  7,  8,  6,  9,  6,  9,  6,\n",
      "         13,  5, 11, 19,  6, 15,  4, 11,  4, 15, 18, 12, 23, 11, 11,  6, 15,  4,\n",
      "         14,  7, 14, 22,  2]])\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "raw_batch = [\n",
    "    (\"Protein1\", \"MVLSPADKTNVKAAWGKVGAHAGEYGAEALERMFLSFPTTKTYFPHFD\"),\n",
    "    (\"Protein2\", \"GAGKLVWGKVNVDEVGGEALGRLLVVYPWTQRFFDSFGDLSNPGAVMGNPK\"),\n",
    "    (\"Protein3\", \"MGDVEKGKKIFVQKCAQCHTVEKGGKHKTGPNEKGQTAFQEALAAKKIL\"),\n",
    "    (\"Protein4\", \"MKLFWLLFTIGFCWAISAEGILHNFSLVRDSQSLEDLGDKLEDLRNL\"),\n",
    "    (\"Protein5\", \"MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPW\")\n",
    "]\n",
    "alphabet=Alphabet.from_architecture(\"ESM-1b\")\n",
    "test_batch=BatchConverter(alphabet)\n",
    "labels, strs, tokens = test_batch(raw_batch)\n",
    "print(labels)\n",
    "print(strs)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "RawMSA = Sequence[Tuple[str, str]]\n",
    "class MSABatchConverter(BatchConverter):\n",
    "    def __call__(self, inputs: Union[Sequence[RawMSA], RawMSA]):\n",
    "        if isinstance(inputs[0][0], str):\n",
    "            # Input is a single MSA\n",
    "            raw_batch: Sequence[RawMSA] = [inputs]  # type: ignore\n",
    "        else:\n",
    "            raw_batch = inputs  # type: ignore\n",
    "\n",
    "        batch_size = len(raw_batch)\n",
    "        max_alignments = max(len(msa) for msa in raw_batch)\n",
    "        max_seqlen = max(len(msa[0][1]) for msa in raw_batch)\n",
    "\n",
    "        tokens = torch.empty(\n",
    "            (\n",
    "                batch_size,\n",
    "                max_alignments,\n",
    "                max_seqlen + int(self.alphabet.prepend_bos) + int(self.alphabet.append_eos),\n",
    "            ),\n",
    "            dtype=torch.int64,\n",
    "        )\n",
    "        tokens.fill_(self.alphabet.padding_idx)\n",
    "        labels = []\n",
    "        strs = []\n",
    "\n",
    "        for i, msa in enumerate(raw_batch):\n",
    "            msa_seqlens = set(len(seq) for _, seq in msa)\n",
    "            if not len(msa_seqlens) == 1:\n",
    "                raise RuntimeError(\n",
    "                    \"Received unaligned sequences for input to MSA, all sequence \"\n",
    "                    \"lengths must be equal.\"\n",
    "                )\n",
    "            msa_labels, msa_strs, msa_tokens = super().__call__(msa)\n",
    "            labels.append(msa_labels)\n",
    "            strs.append(msa_strs)\n",
    "            tokens[i, : msa_tokens.size(0), : msa_tokens.size(1)] = msa_tokens\n",
    "\n",
    "        return labels, strs, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Seq1', 'Seq2'], ['SeqA', 'SeqB']]\n",
      "[['MVLSPADKTNVKAAWGKVGAHAGEYGAEALERMFLS', 'GAGKLVWGKVNVDEVGGEALGRLLVVYPWTQRFFDS'], ['MGDVEKGKKIFVQKCAQCHTVEKGGKHKTGPNEKGQ', 'MKLFWLLFTIGFCWAISAEGILHNFSLVRDSQSLED']]\n",
      "tensor([[[ 0, 20,  7,  4,  8, 14,  5, 13, 15, 11, 17,  7, 15,  5,  5, 22,  6,\n",
      "          15,  7,  6,  5, 21,  5,  6,  9, 19,  6,  5,  9,  5,  4,  9, 10, 20,\n",
      "          18,  4,  8,  2],\n",
      "         [ 0,  6,  5,  6, 15,  4,  7, 22,  6, 15,  7, 17,  7, 13,  9,  7,  6,\n",
      "           6,  9,  5,  4,  6, 10,  4,  4,  7,  7, 19, 14, 22, 11, 16, 10, 18,\n",
      "          18, 13,  8,  2]],\n",
      "\n",
      "        [[ 0, 20,  6, 13,  7,  9, 15,  6, 15, 15, 12, 18,  7, 16, 15, 23,  5,\n",
      "          16, 23, 21, 11,  7,  9, 15,  6,  6, 15, 21, 15, 11,  6, 14, 17,  9,\n",
      "          15,  6, 16,  2],\n",
      "         [ 0, 20, 15,  4, 18, 22,  4,  4, 18, 11, 12,  6, 18, 23, 22,  5, 12,\n",
      "           8,  5,  9,  6, 12,  4, 21, 17, 18,  8,  4,  7, 10, 13,  8, 16,  8,\n",
      "           4,  9, 13,  2]]])\n"
     ]
    }
   ],
   "source": [
    "inputs = [\n",
    "    [\n",
    "        (\"Seq1\", \"MVLSPADKTNVKAAWGKVGAHAGEYGAEALERMFLS\"),\n",
    "        (\"Seq2\", \"GAGKLVWGKVNVDEVGGEALGRLLVVYPWTQRFFDS\")\n",
    "    ],\n",
    "    [\n",
    "        (\"SeqA\", \"MGDVEKGKKIFVQKCAQCHTVEKGGKHKTGPNEKGQ\"),\n",
    "        (\"SeqB\", \"MKLFWLLFTIGFCWAISAEGILHNFSLVRDSQSLED\")\n",
    "    ]\n",
    "]\n",
    "\n",
    "test_msa_batch=MSABatchConverter(alphabet)\n",
    "labels, strs, tokens = test_msa_batch(inputs)\n",
    "print(labels)\n",
    "print(strs)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use already curated data within ESM model, we could use this class\n",
    "import itertools\n",
    "import os\n",
    "from typing import Sequence, Tuple, List, Union\n",
    "import pickle\n",
    "import re\n",
    "import shutil\n",
    "import torch\n",
    "class ESMStructuralSplitDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Structural Split Dataset as described in section A.10 of the supplement of our paper.\n",
    "    https://doi.org/10.1101/622803\n",
    "\n",
    "    We use the full version of SCOPe 2.07, clustered at 90% sequence identity,\n",
    "    generated on January 23, 2020.\n",
    "\n",
    "    For each SCOPe domain:\n",
    "        - We extract the sequence from the corresponding PDB file\n",
    "        - We extract the 3D coordinates of the Carbon beta atoms, aligning them\n",
    "          to the sequence. We put NaN where Cb atoms are missing.\n",
    "        - From the 3D coordinates, we calculate a pairwise distance map, based\n",
    "          on L2 distance\n",
    "        - We use DSSP to generate secondary structure labels for the corresponding\n",
    "          PDB file. This is also aligned to the sequence. We put - where SSP\n",
    "          labels are missing.\n",
    "\n",
    "    For each SCOPe classification level of family/superfamily/fold (in order of difficulty),\n",
    "    we have split the data into 5 partitions for cross validation. These are provided\n",
    "    in a downloaded splits folder, in the format:\n",
    "            splits/{split_level}/{cv_partition}/{train|valid}.txt\n",
    "    where train is the partition and valid is the concatentation of the remaining 4.\n",
    "\n",
    "    For each SCOPe domain, we provide a pkl dump that contains:\n",
    "        - seq    : The domain sequence, stored as an L-length string\n",
    "        - ssp    : The secondary structure labels, stored as an L-length string\n",
    "        - dist   : The distance map, stored as an LxL numpy array\n",
    "        - coords : The 3D coordinates, stored as an Lx3 numpy array\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    base_folder = \"structural-data\"\n",
    "    file_list = [\n",
    "        #  url  tar filename   filename      MD5 Hash\n",
    "        (\n",
    "            \"https://dl.fbaipublicfiles.com/fair-esm/structural-data/splits.tar.gz\",\n",
    "            \"splits.tar.gz\",\n",
    "            \"splits\",\n",
    "            \"456fe1c7f22c9d3d8dfe9735da52411d\",\n",
    "        ),\n",
    "        (\n",
    "            \"https://dl.fbaipublicfiles.com/fair-esm/structural-data/pkl.tar.gz\",\n",
    "            \"pkl.tar.gz\",\n",
    "            \"pkl\",\n",
    "            \"644ea91e56066c750cd50101d390f5db\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        split_level,\n",
    "        cv_partition,\n",
    "        split,\n",
    "        root_path=os.path.expanduser(\"~/.cache/torch/data/esm\"),\n",
    "        download=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert split in [\n",
    "            \"train\",\n",
    "            \"valid\",\n",
    "        ], \"train_valid must be 'train' or 'valid'\"\n",
    "        self.root_path = root_path\n",
    "        self.base_path = os.path.join(self.root_path, self.base_folder)\n",
    "\n",
    "        # check if root path has what you need or else download it\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        self.split_file = os.path.join(\n",
    "            self.base_path, \"splits\", split_level, cv_partition, f\"{split}.txt\"\n",
    "        )\n",
    "        self.pkl_dir = os.path.join(self.base_path, \"pkl\")\n",
    "        self.names = []\n",
    "        with open(self.split_file) as f:\n",
    "            self.names = f.read().splitlines()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def _check_exists(self) -> bool:\n",
    "        for (_, _, filename, _) in self.file_list:\n",
    "            fpath = os.path.join(self.base_path, filename)\n",
    "            if not os.path.exists(fpath) or not os.path.isdir(fpath):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def download(self):\n",
    "\n",
    "        if self._check_exists():\n",
    "            print(\"Files already downloaded and verified\")\n",
    "            return\n",
    "\n",
    "        from torchvision.datasets.utils import download_url\n",
    "\n",
    "        for url, tar_filename, filename, md5_hash in self.file_list:\n",
    "            download_path = os.path.join(self.base_path, tar_filename)\n",
    "            download_url(url=url, root=self.base_path, filename=tar_filename, md5=md5_hash)\n",
    "            shutil.unpack_archive(download_path, self.base_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a dict with the following entires\n",
    "         - seq : Str (domain sequence)\n",
    "         - ssp : Str (SSP labels)\n",
    "         - dist : np.array (distance map)\n",
    "         - coords : np.array (3D coordinates)\n",
    "        \"\"\"\n",
    "        name = self.names[idx]\n",
    "        pkl_fname = os.path.join(self.pkl_dir, name[1:3], f\"{name}.pkl\")\n",
    "        with open(pkl_fname, \"rb\") as f:\n",
    "            obj = pickle.load(f)\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://dl.fbaipublicfiles.com/fair-esm/structural-data/splits.tar.gz to /home/yunyao/.cache/torch/data/esm/structural-data/splits.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3183408357cb44b88dbfb908cdeb86bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/744059 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://dl.fbaipublicfiles.com/fair-esm/structural-data/pkl.tar.gz to /home/yunyao/.cache/torch/data/esm/structural-data/pkl.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53e83484f0b466981ae155a588b843c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2601334121 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# example: https://github.com/facebookresearch/esm/blob/main/examples/esm_structural_dataset.ipynb\n",
    "for split_level in ['family', 'superfamily', 'fold']:\n",
    "    for cv_partition in ['0', '1', '2', '3', '4']:\n",
    "        esm_structural_train = ESMStructuralSplitDataset(\n",
    "            split_level=split_level, \n",
    "            cv_partition=cv_partition, \n",
    "            split='train', \n",
    "            root_path = os.path.expanduser('~/.cache/torch/data/esm'),\n",
    "            download=False # change it to False after download\n",
    "        )\n",
    "        esm_structural_valid = ESMStructuralSplitDataset(\n",
    "            split_level=split_level, \n",
    "            cv_partition=cv_partition, \n",
    "            split='valid', \n",
    "            root_path = os.path.expanduser('~/.cache/torch/data/esm'),\n",
    "            download=False\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'seq': 'MKKVVAVVKLQLPAGKATPAPPVGPALGQHGANIMEFVKAFNAATANMGDAIVPVEITIYADRSFTFVTK',\n",
       " 'ssp': '---EEEEEEEEEETT---SSTTHHHHHHTTT--HHHHHHHHHHHHGGG-S-EEEEEEEEETTS-EEEEE-',\n",
       " 'dist': array([[ 0.       ,  5.6424503,  6.783698 , ..., 18.10466  , 23.448418 ,\n",
       "         25.104635 ],\n",
       "        [ 5.6424503,  0.       ,  5.4445224, ..., 15.816407 , 20.44215  ,\n",
       "         22.99304  ],\n",
       "        [ 6.783698 ,  5.4445224,  0.       , ..., 18.53912  , 23.44243  ,\n",
       "         25.21559  ],\n",
       "        ...,\n",
       "        [18.10466  , 15.816407 , 18.53912  , ...,  0.       ,  5.889919 ,\n",
       "          7.2993445],\n",
       "        [23.448418 , 20.44215  , 23.44243  , ...,  5.889919 ,  0.       ,\n",
       "          5.500579 ],\n",
       "        [25.104635 , 22.99304  , 25.21559  , ...,  7.2993445,  5.500579 ,\n",
       "          0.       ]], dtype=float32),\n",
       " 'coords': array([[31.596, 17.872, 30.571],\n",
       "        [27.18 , 15.156, 32.798],\n",
       "        [31.386, 11.782, 33.552],\n",
       "        [28.759,  9.174, 29.113],\n",
       "        [27.723,  5.838, 33.829],\n",
       "        [25.195,  3.299, 30.532],\n",
       "        [25.08 ,  4.954, 25.195],\n",
       "        [20.298,  2.287, 27.193],\n",
       "        [20.944,  1.097, 21.413],\n",
       "        [15.443,  0.689, 23.705],\n",
       "        [13.271, -0.901, 18.92 ],\n",
       "        [10.166,  1.417, 23.225],\n",
       "        [ 5.367, -0.342, 21.382],\n",
       "        [ 4.88 ,  4.933, 19.158],\n",
       "        [ 3.42 ,  5.652, 23.5  ],\n",
       "        [ 2.958,  1.179, 24.291],\n",
       "        [ 7.806,  3.184, 27.352],\n",
       "        [ 5.468, -0.362, 31.143],\n",
       "        [ 8.572,  0.698, 35.152],\n",
       "        [ 7.979, -4.155, 32.314],\n",
       "        [12.564, -6.289, 31.258],\n",
       "        [14.623, -3.65 , 27.449],\n",
       "        [12.248,  0.59 , 30.197],\n",
       "        [13.691, -1.087, 34.042],\n",
       "        [17.038, -4.471, 33.606],\n",
       "        [18.153, -0.892, 29.806],\n",
       "        [16.936,  3.189, 32.788],\n",
       "        [17.88 ,  1.162, 36.571],\n",
       "        [21.868, -1.605, 35.849],\n",
       "        [23.144,  3.448, 34.871],\n",
       "        [20.245,  4.717, 38.546],\n",
       "        [18.492,  6.44 , 34.326],\n",
       "        [14.359,  8.909, 37.186],\n",
       "        [12.553,  3.361, 35.765],\n",
       "        [ 8.573,  6.572, 36.776],\n",
       "        [11.803, 10.627, 34.877],\n",
       "        [13.168,  7.059, 30.965],\n",
       "        [ 8.152,  5.155, 31.616],\n",
       "        [ 6.488, 10.122, 32.121],\n",
       "        [10.512, 11.676, 29.056],\n",
       "        [ 9.915,  7.207, 26.116],\n",
       "        [ 4.725,  7.576, 27.074],\n",
       "        [ 5.493, 13.016, 26.664],\n",
       "        [ 9.129, 11.75 , 22.992],\n",
       "        [ 6.792,  7.938, 21.023],\n",
       "        [ 1.54 ,  9.114, 23.172],\n",
       "        [ 0.893, 12.12 , 18.866],\n",
       "        [ 4.377,  7.92 , 16.695],\n",
       "        [ 0.251,  5.586, 16.755],\n",
       "        [ 0.352,  0.955, 15.508],\n",
       "        [ 5.463,  4.01 , 15.109],\n",
       "        [ 8.687, -0.359, 16.255],\n",
       "        [ 9.915,  4.842, 18.715],\n",
       "        [15.161,  3.761, 16.725],\n",
       "        [14.353,  5.552, 22.151],\n",
       "        [20.095,  6.28 , 21.604],\n",
       "        [18.463,  6.14 , 27.278],\n",
       "        [23.265,  9.492, 26.824],\n",
       "        [22.593,  6.712, 31.979],\n",
       "        [25.174, 11.795, 33.126],\n",
       "        [28.782, 10.258, 36.678],\n",
       "        [24.67 , 13.54 , 38.279],\n",
       "        [21.513,  8.996, 38.598],\n",
       "        [20.833, 13.526, 34.758],\n",
       "        [18.885,  9.716, 31.277],\n",
       "        [20.097, 13.392, 27.248],\n",
       "        [16.37 ,  9.689, 25.446],\n",
       "        [18.371, 10.869, 20.381],\n",
       "        [12.749,  9.324, 19.546],\n",
       "        [14.994,  7.448, 14.888]], dtype=float32)}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(esm_structural_train))\n",
    "demo=esm_structural_train[0]\n",
    "demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
