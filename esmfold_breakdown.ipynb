{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESMFold breakdown\n",
    "It took ouput from ESM (sequence reperesentation, attention matrix) as input\n",
    "into folding trunk then structure module to predict structure.  \n",
    "\n",
    "utility modules (misc.py)\n",
    "read seqence module (encode_sequence): input sequence, add chain_linker( optional, default 'G'*25), add residue_index_offset (add a jump in index if in different chain)\n",
    "return:encoded (residue token), residx, linker_mask, chain_index\n",
    "\n",
    "read seqence in batch \n",
    "\n",
    "output_to_pdb: input: output from model (in dict format) output: pdb files\n",
    "\n",
    "Attention: to calcualte attention in Transformer\n",
    "\n",
    "Dropout: \n",
    "\n",
    "SequencToPair:\n",
    "\n",
    "PairToSequence:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is how differnt ESM model get loaded into ESMFold model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_model(model_name):\n",
    "    if model_name.endswith(\".pt\"):  # local, treat as filepath\n",
    "        model_path = Path(model_name)\n",
    "        model_data = torch.load(str(model_path), map_location=\"cpu\")\n",
    "    else:  # load from hub\n",
    "        url = f\"https://dl.fbaipublicfiles.com/fair-esm/models/{model_name}.pt\"\n",
    "        model_data = torch.hub.load_state_dict_from_url(url, progress=False, map_location=\"cpu\")\n",
    "\n",
    "    cfg = model_data[\"cfg\"][\"model\"]\n",
    "    model_state = model_data[\"model\"]  # contain weights and bais\n",
    "    model = ESMFold(esmfold_config=cfg)\n",
    "\n",
    "    expected_keys = set(model.state_dict().keys())\n",
    "    found_keys = set(model_state.keys())\n",
    "\n",
    "    missing_essential_keys = []  # make sure keys are compatiable before load parameters\n",
    "    for missing_key in expected_keys - found_keys:\n",
    "        if not missing_key.startswith(\"esm.\"):\n",
    "            missing_essential_keys.append(missing_key)\n",
    "\n",
    "    if missing_essential_keys:\n",
    "        raise RuntimeError(f\"Keys '{', '.join(missing_essential_keys)}' are missing.\")\n",
    "\n",
    "    model.load_state_dict(model_state, strict=False) # \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triangluar self attention block\n",
    "Inputs:\n",
    "sequence_state: Shape (B, L, sequence_state_dim) (batch, sequence length, feature dimension).  \n",
    "pairwise_state: Shape (B, L, L, pairwise_state_dim) (batch, pairwise interactions, feature dimension).  \n",
    "mask: Optional (B, L) mask indicating valid sequence positions.  \n",
    "\n",
    "operation:\n",
    "Sequence State Updates:\n",
    "Applies pair-to-sequence projection (self.pair_to_sequence) to derive biases for attention.\n",
    "Applies self-attention (self.seq_attention) with the computed biases.\n",
    "Adds residual connections and applies an MLP (self.mlp_seq) to the sequence state.\n",
    "Pairwise State Updates:\n",
    "\n",
    "Updates pairwise state with sequence-to-pair projections (self.sequence_to_pair).\n",
    "Applies triangular multiplications (outgoing and incoming).\n",
    "Applies triangular attention (starting and ending nodes).\n",
    "Adds residual connections and applies an MLP (self.mlp_pair) to the pairwise state.\n",
    "Returns the updated sequence and pairwise states.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code fore sequence_to_pair\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SequenceToPair(nn.Module):\n",
    "    def __init__(self, sequence_state_dim, inner_dim, pairwise_state_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(sequence_state_dim)\n",
    "        self.proj = nn.Linear(sequence_state_dim, inner_dim * 2, bias=True)\n",
    "        self.o_proj = nn.Linear(2 * inner_dim, pairwise_state_dim, bias=True)\n",
    "\n",
    "        torch.nn.init.zeros_(self.proj.bias)\n",
    "        torch.nn.init.zeros_(self.o_proj.bias)\n",
    "\n",
    "    def forward(self, sequence_state):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          sequence_state: B x L x sequence_state_dim\n",
    "\n",
    "        Output:\n",
    "          pairwise_state: B x L x L x pairwise_state_dim\n",
    "\n",
    "        Intermediate state:\n",
    "          B x L x L x 2*inner_dim\n",
    "        \"\"\"\n",
    "\n",
    "        assert len(sequence_state.shape) == 3\n",
    "\n",
    "        s = self.layernorm(sequence_state)\n",
    "        s = self.proj(s)\n",
    "        q, k = s.chunk(2, dim=-1)\n",
    "\n",
    "        prod = q[:, None, :, :] * k[:, :, None, :]\n",
    "        diff = q[:, None, :, :] - k[:, :, None, :]\n",
    "\n",
    "        x = torch.cat([prod, diff], dim=-1)\n",
    "        x = self.o_proj(x)\n",
    "\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is code for pair to sequence\n",
    "# use pair info as bais for transformer attention matrix\n",
    "class PairToSequence(nn.Module):\n",
    "    def __init__(self, pairwise_state_dim, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(pairwise_state_dim)\n",
    "        self.linear = nn.Linear(pairwise_state_dim, num_heads, bias=False)\n",
    "\n",
    "    def forward(self, pairwise_state):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          pairwise_state: B x L x L x pairwise_state_dim\n",
    "\n",
    "        Output:\n",
    "          pairwise_bias: B x L x L x num_heads\n",
    "        \"\"\"\n",
    "        assert len(pairwise_state.shape) == 4\n",
    "        z = self.layernorm(pairwise_state)\n",
    "        pairwise_bias = self.linear(z)\n",
    "        return pairwise_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# costomized Dropout so that it consistantly applied to same dimension \n",
    "# for exmaple, along 0, so that each batch has the same dropout to improve learning\n",
    "import typing as T\n",
    "class Dropout(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of dropout with the ability to share the dropout mask\n",
    "    along a particular dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, r: float, batch_dim: T.Union[int, T.List[int]]):\n",
    "        super(Dropout, self).__init__()\n",
    "\n",
    "        self.r = r\n",
    "        if type(batch_dim) == int:\n",
    "            batch_dim = [batch_dim]\n",
    "        self.batch_dim = batch_dim\n",
    "        self.dropout = nn.Dropout(self.r)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        shape = list(x.shape)\n",
    "        if self.batch_dim is not None:\n",
    "            for bd in self.batch_dim:\n",
    "                shape[bd] = 1\n",
    "        return x * self.dropout(x.new_ones(shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor:\n",
      "tensor([[[ 1.,  2.],\n",
      "         [ 3.,  4.],\n",
      "         [ 5.,  6.]],\n",
      "\n",
      "        [[ 7.,  8.],\n",
      "         [ 9., 10.],\n",
      "         [11., 12.]],\n",
      "\n",
      "        [[13., 14.],\n",
      "         [15., 16.],\n",
      "         [17., 18.]],\n",
      "\n",
      "        [[19., 20.],\n",
      "         [21., 22.],\n",
      "         [23., 24.]]])\n",
      "\n",
      "Output Tensor:\n",
      "tensor([[[ 2.,  4.],\n",
      "         [ 6.,  8.],\n",
      "         [ 0., 12.]],\n",
      "\n",
      "        [[14., 16.],\n",
      "         [18., 20.],\n",
      "         [ 0., 24.]],\n",
      "\n",
      "        [[26., 28.],\n",
      "         [30., 32.],\n",
      "         [ 0., 36.]],\n",
      "\n",
      "        [[38., 40.],\n",
      "         [42., 44.],\n",
      "         [ 0., 48.]]])\n"
     ]
    }
   ],
   "source": [
    "# example of dropout function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define input tensor (batch size = 4, sequence length = 3, feature dim = 2)\n",
    "x = torch.tensor(\n",
    "    [[[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]],\n",
    "     [[7.0, 8.0], [9.0, 10.0], [11.0, 12.0]],\n",
    "     [[13.0, 14.0], [15.0, 16.0], [17.0, 18.0]],\n",
    "     [[19.0, 20.0], [21.0, 22.0], [23.0, 24.0]]]\n",
    ")\n",
    "\n",
    "# Create the dropout layer with r=0.5 and shared mask along batch dimension\n",
    "dropout_layer = Dropout(r=0.5, batch_dim=0)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Apply dropout\n",
    "output = dropout_layer(x)\n",
    "\n",
    "print(\"Input Tensor:\")\n",
    "print(x)\n",
    "print(\"\\nOutput Tensor:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special Attetnion units\n",
    "#1. add bias from pair info 2.with gating option 3. with masking option\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, head_width, gated=False):\n",
    "        super().__init__()\n",
    "        assert embed_dim == num_heads * head_width\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_width = head_width\n",
    "\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim * 3, bias=False)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "        self.gated = gated\n",
    "        if gated:\n",
    "            self.g_proj = nn.Linear(embed_dim, embed_dim)\n",
    "            torch.nn.init.zeros_(self.g_proj.weight)\n",
    "            torch.nn.init.ones_(self.g_proj.bias)\n",
    "\n",
    "        self.rescale_factor = self.head_width**-0.5\n",
    "\n",
    "        torch.nn.init.zeros_(self.o_proj.bias)\n",
    "\n",
    "    def forward(self, x, mask=None, bias=None, indices=None):\n",
    "        \"\"\"\n",
    "        Basic self attention with optional mask and external pairwise bias.\n",
    "        To handle sequences of different lengths, use mask.\n",
    "\n",
    "        Inputs:\n",
    "          x: batch of input sequneces (.. x L x C)\n",
    "          mask: batch of boolean masks where 1=valid, 0=padding position (.. x L_k). optional.\n",
    "          bias: batch of scalar pairwise attention biases (.. x Lq x Lk x num_heads). optional.\n",
    "\n",
    "        Outputs:\n",
    "          sequence projection (B x L x embed_dim), attention maps (B x L x L x num_heads)\n",
    "        \"\"\"\n",
    "\n",
    "        t = rearrange(self.proj(x), \"... l (h c) -> ... h l c\", h=self.num_heads)\n",
    "        q, k, v = t.chunk(3, dim=-1)\n",
    "\n",
    "        q = self.rescale_factor * q\n",
    "        a = torch.einsum(\"...qc,...kc->...qk\", q, k)\n",
    "\n",
    "        # Add external attention bias.\n",
    "        if bias is not None:\n",
    "            a = a + rearrange(bias, \"... lq lk h -> ... h lq lk\")\n",
    "\n",
    "        # Do not attend to padding tokens.\n",
    "        if mask is not None:\n",
    "            mask = repeat(\n",
    "                mask, \"... lk -> ... h lq lk\", h=self.num_heads, lq=q.shape[-2]\n",
    "            )\n",
    "            a = a.masked_fill(mask == False, -np.inf)\n",
    "\n",
    "        a = F.softmax(a, dim=-1)\n",
    "\n",
    "        y = torch.einsum(\"...hqk,...hkc->...qhc\", a, v)\n",
    "        y = rearrange(y, \"... h c -> ... (h c)\", h=self.num_heads)\n",
    "\n",
    "        if self.gated:\n",
    "            y = self.g_proj(x).sigmoid() * y\n",
    "        y = self.o_proj(y)\n",
    "\n",
    "        return y, rearrange(a, \"... lq lk h -> ... h lq lk\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch(env)",
   "language": "python",
   "name": "pytroch_kern"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
