{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESMFold breakdown\n",
    "It took ouput from ESM (sequence reperesentation, attention matrix) as input\n",
    "into folding trunk then structure module to predict structure.  \n",
    "\n",
    "utility modules (misc.py)\n",
    "read seqence module (encode_sequence): input sequence, add chain_linker( optional, default 'G'*25), add residue_index_offset (add a jump in index if in different chain)\n",
    "return:encoded (residue token), residx, linker_mask, chain_index\n",
    "\n",
    "read seqence in batch \n",
    "\n",
    "output_to_pdb: input: output from model (in dict format) output: pdb files\n",
    "\n",
    "Attention: to calcualte attention in Transformer\n",
    "\n",
    "Dropout: \n",
    "\n",
    "SequencToPair:\n",
    "\n",
    "PairToSequence:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is how differnt ESM model get loaded into ESMFold model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_model(model_name):\n",
    "    if model_name.endswith(\".pt\"):  # local, treat as filepath\n",
    "        model_path = Path(model_name)\n",
    "        model_data = torch.load(str(model_path), map_location=\"cpu\")\n",
    "    else:  # load from hub\n",
    "        url = f\"https://dl.fbaipublicfiles.com/fair-esm/models/{model_name}.pt\"\n",
    "        model_data = torch.hub.load_state_dict_from_url(url, progress=False, map_location=\"cpu\")\n",
    "\n",
    "    cfg = model_data[\"cfg\"][\"model\"]\n",
    "    model_state = model_data[\"model\"]  # contain weights and bais\n",
    "    model = ESMFold(esmfold_config=cfg)\n",
    "\n",
    "    expected_keys = set(model.state_dict().keys())\n",
    "    found_keys = set(model_state.keys())\n",
    "\n",
    "    missing_essential_keys = []  # make sure keys are compatiable before load parameters\n",
    "    for missing_key in expected_keys - found_keys:\n",
    "        if not missing_key.startswith(\"esm.\"):\n",
    "            missing_essential_keys.append(missing_key)\n",
    "\n",
    "    if missing_essential_keys:\n",
    "        raise RuntimeError(f\"Keys '{', '.join(missing_essential_keys)}' are missing.\")\n",
    "\n",
    "    model.load_state_dict(model_state, strict=False) # \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triangluar self attention block\n",
    "Inputs:\n",
    "sequence_state: Shape (B, L, sequence_state_dim) (batch, sequence length, feature dimension).  \n",
    "pairwise_state: Shape (B, L, L, pairwise_state_dim) (batch, pairwise interactions, feature dimension).  \n",
    "mask: Optional (B, L) mask indicating valid sequence positions.  \n",
    "![Folding Truk](folding_trunk.png)\n",
    "\n",
    "### operation ( very close to Alphafold2 Evoformer module):\n",
    "### (use to update seq and pair representation by passing message between there two)\n",
    "### (when update pair representation, it contains triangular multiplication and triangular attention)\n",
    "triangular multiplication: a way of message passing, if i, k interact, j, k interact, i, j should interact. (This message passing is encoded through element-wise multiplication)\n",
    "triangular attention: simply a way to calcualte inter residue attention  in column or row wise\n",
    "Sequence State Updates:\n",
    "Applies pair-to-sequence projection (self.pair_to_sequence) to derive biases for attention.\n",
    "Applies self-attention (self.seq_attention) with the computed biases.\n",
    "Adds residual connections and applies an MLP (self.mlp_seq) to the sequence state.\n",
    "Pairwise State Updates:\n",
    "\n",
    "Updates pairwise state with sequence-to-pair projections (self.sequence_to_pair).\n",
    "Applies triangular multiplications (outgoing and incoming).\n",
    "Applies triangular attention (starting and ending nodes).\n",
    "Adds residual connections and applies an MLP (self.mlp_pair) to the pairwise state.\n",
    "Returns the updated sequence and pairwise states.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We illusrate some technique details with code below\n",
    "1. how sequence info pass to pair: SequenceToPair\n",
    "2. how pair info pass to sequence : PairToSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code fore sequence_to_pair\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SequenceToPair(nn.Module):\n",
    "    def __init__(self, sequence_state_dim, inner_dim, pairwise_state_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(sequence_state_dim)\n",
    "        self.proj = nn.Linear(sequence_state_dim, inner_dim * 2, bias=True)\n",
    "        self.o_proj = nn.Linear(2 * inner_dim, pairwise_state_dim, bias=True)\n",
    "\n",
    "        torch.nn.init.zeros_(self.proj.bias)\n",
    "        torch.nn.init.zeros_(self.o_proj.bias)\n",
    "\n",
    "    def forward(self, sequence_state):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          sequence_state: B x L x sequence_state_dim\n",
    "\n",
    "        Output:\n",
    "          pairwise_state: B x L x L x pairwise_state_dim\n",
    "\n",
    "        Intermediate state:\n",
    "          B x L x L x 2*inner_dim\n",
    "        \"\"\"\n",
    "\n",
    "        assert len(sequence_state.shape) == 3\n",
    "\n",
    "        s = self.layernorm(sequence_state)\n",
    "        s = self.proj(s)\n",
    "        q, k = s.chunk(2, dim=-1)\n",
    "\n",
    "        prod = q[:, None, :, :] * k[:, :, None, :]\n",
    "        diff = q[:, None, :, :] - k[:, :, None, :]\n",
    "\n",
    "        x = torch.cat([prod, diff], dim=-1)\n",
    "        x = self.o_proj(x)\n",
    "\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is code for pair to sequence\n",
    "# use pair info as bais for transformer attention matrix\n",
    "class PairToSequence(nn.Module):\n",
    "    def __init__(self, pairwise_state_dim, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(pairwise_state_dim)\n",
    "        self.linear = nn.Linear(pairwise_state_dim, num_heads, bias=False)\n",
    "\n",
    "    def forward(self, pairwise_state):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          pairwise_state: B x L x L x pairwise_state_dim\n",
    "\n",
    "        Output:\n",
    "          pairwise_bias: B x L x L x num_heads\n",
    "        \"\"\"\n",
    "        assert len(pairwise_state.shape) == 4\n",
    "        z = self.layernorm(pairwise_state)\n",
    "        pairwise_bias = self.linear(z)\n",
    "        return pairwise_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# costomized Dropout so that it consistantly applied to same dimension \n",
    "# for exmaple, along 0, so that each batch has the same dropout to improve learning\n",
    "import typing as T\n",
    "class Dropout(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of dropout with the ability to share the dropout mask\n",
    "    along a particular dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, r: float, batch_dim: T.Union[int, T.List[int]]):\n",
    "        super(Dropout, self).__init__()\n",
    "\n",
    "        self.r = r\n",
    "        if type(batch_dim) == int:\n",
    "            batch_dim = [batch_dim]\n",
    "        self.batch_dim = batch_dim\n",
    "        self.dropout = nn.Dropout(self.r)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        shape = list(x.shape)\n",
    "        if self.batch_dim is not None:\n",
    "            for bd in self.batch_dim:\n",
    "                shape[bd] = 1\n",
    "        return x * self.dropout(x.new_ones(shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor:\n",
      "tensor([[[ 1.,  2.],\n",
      "         [ 3.,  4.],\n",
      "         [ 5.,  6.]],\n",
      "\n",
      "        [[ 7.,  8.],\n",
      "         [ 9., 10.],\n",
      "         [11., 12.]],\n",
      "\n",
      "        [[13., 14.],\n",
      "         [15., 16.],\n",
      "         [17., 18.]],\n",
      "\n",
      "        [[19., 20.],\n",
      "         [21., 22.],\n",
      "         [23., 24.]]])\n",
      "\n",
      "Output Tensor:\n",
      "tensor([[[ 2.,  4.],\n",
      "         [ 6.,  8.],\n",
      "         [ 0., 12.]],\n",
      "\n",
      "        [[14., 16.],\n",
      "         [18., 20.],\n",
      "         [ 0., 24.]],\n",
      "\n",
      "        [[26., 28.],\n",
      "         [30., 32.],\n",
      "         [ 0., 36.]],\n",
      "\n",
      "        [[38., 40.],\n",
      "         [42., 44.],\n",
      "         [ 0., 48.]]])\n"
     ]
    }
   ],
   "source": [
    "# example of dropout function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define input tensor (batch size = 4, sequence length = 3, feature dim = 2)\n",
    "x = torch.tensor(\n",
    "    [[[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]],\n",
    "     [[7.0, 8.0], [9.0, 10.0], [11.0, 12.0]],\n",
    "     [[13.0, 14.0], [15.0, 16.0], [17.0, 18.0]],\n",
    "     [[19.0, 20.0], [21.0, 22.0], [23.0, 24.0]]]\n",
    ")\n",
    "\n",
    "# Create the dropout layer with r=0.5 and shared mask along batch dimension\n",
    "dropout_layer = Dropout(r=0.5, batch_dim=0)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Apply dropout\n",
    "output = dropout_layer(x)\n",
    "\n",
    "print(\"Input Tensor:\")\n",
    "print(x)\n",
    "print(\"\\nOutput Tensor:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special Attetnion units\n",
    "#1. add bias from pair info 2.with gating option 3. with masking option\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, head_width, gated=False):\n",
    "        super().__init__()\n",
    "        assert embed_dim == num_heads * head_width\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_width = head_width\n",
    "\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim * 3, bias=False)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "        self.gated = gated\n",
    "        if gated:\n",
    "            self.g_proj = nn.Linear(embed_dim, embed_dim)\n",
    "            torch.nn.init.zeros_(self.g_proj.weight)\n",
    "            torch.nn.init.ones_(self.g_proj.bias)\n",
    "\n",
    "        self.rescale_factor = self.head_width**-0.5\n",
    "\n",
    "        torch.nn.init.zeros_(self.o_proj.bias)\n",
    "\n",
    "    def forward(self, x, mask=None, bias=None, indices=None):\n",
    "        \"\"\"\n",
    "        Basic self attention with optional mask and external pairwise bias.\n",
    "        To handle sequences of different lengths, use mask.\n",
    "\n",
    "        Inputs:\n",
    "          x: batch of input sequneces (.. x L x C)\n",
    "          mask: batch of boolean masks where 1=valid, 0=padding position (.. x L_k). optional.\n",
    "          bias: batch of scalar pairwise attention biases (.. x Lq x Lk x num_heads). optional.\n",
    "\n",
    "        Outputs:\n",
    "          sequence projection (B x L x embed_dim), attention maps (B x L x L x num_heads)\n",
    "        \"\"\"\n",
    "\n",
    "        t = rearrange(self.proj(x), \"... l (h c) -> ... h l c\", h=self.num_heads)\n",
    "        q, k, v = t.chunk(3, dim=-1)\n",
    "\n",
    "        q = self.rescale_factor * q\n",
    "        a = torch.einsum(\"...qc,...kc->...qk\", q, k)\n",
    "\n",
    "        # Add external attention bias.\n",
    "        if bias is not None:\n",
    "            a = a + rearrange(bias, \"... lq lk h -> ... h lq lk\")\n",
    "\n",
    "        # Do not attend to padding tokens.\n",
    "        if mask is not None:\n",
    "            mask = repeat(\n",
    "                mask, \"... lk -> ... h lq lk\", h=self.num_heads, lq=q.shape[-2]\n",
    "            )\n",
    "            a = a.masked_fill(mask == False, -np.inf)\n",
    "\n",
    "        a = F.softmax(a, dim=-1)\n",
    "\n",
    "        y = torch.einsum(\"...hqk,...hkc->...qhc\", a, v)\n",
    "        y = rearrange(y, \"... h c -> ... (h c)\", h=self.num_heads)\n",
    "\n",
    "        if self.gated:\n",
    "            y = self.g_proj(x).sigmoid() * y\n",
    "        y = self.o_proj(y)\n",
    "\n",
    "        return y, rearrange(a, \"... lq lk h -> ... h lq lk\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distogram (used for calcualte training loss)\n",
    "# calculate CB position based on CA N C \n",
    "# calcuculate interresdiuce CB distance, then bin the distance (output L, L, 1)\n",
    "# for example  bin 2 with [1, 2, 3, 4], the output is 2\n",
    "\n",
    "def distogram(coords, min_bin, max_bin, num_bins):\n",
    "    # Coords are [... L x 3 x 3], where it's [N, CA, C] x 3 coordinates.\n",
    "    boundaries = torch.linspace(\n",
    "        min_bin,\n",
    "        max_bin,\n",
    "        num_bins - 1,\n",
    "        device=coords.device,\n",
    "    )\n",
    "    boundaries = boundaries**2\n",
    "    N, CA, C = [x.squeeze(-2) for x in coords.chunk(3, dim=-2)]\n",
    "    # Infer CB coordinates.\n",
    "    b = CA - N\n",
    "    c = C - CA\n",
    "    a = b.cross(c, dim=-1)\n",
    "    CB = -0.58273431 * a + 0.56802827 * b - 0.54067466 * c + CA\n",
    "    dists = (CB[..., None, :, :] - CB[..., :, None, :]).pow(2).sum(dim=-1, keepdims=True)\n",
    "    bins = torch.sum(dists > boundaries, dim=-1)  # [..., L, L]\n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to inspect the structure module: how seq, pair represenation used for building 3D structure\n",
    "# this structure module comes from openfold : https://github.com/aqlaboratory/openfold/blob/main/openfold/model/structure_module.py\n",
    "#detailed explanation of structure module comes from Alphafold2 paper(Algorithm 20): https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-021-03819-2/MediaObjects/41586_2021_3819_MOESM1_ESM.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assemble all commponents\n",
    "class FoldingTrunk(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.cfg = FoldingTrunkConfig(**kwargs)\n",
    "        assert self.cfg.max_recycles > 0\n",
    "\n",
    "        c_s = self.cfg.sequence_state_dim\n",
    "        c_z = self.cfg.pairwise_state_dim\n",
    "\n",
    "        assert c_s % self.cfg.sequence_head_width == 0\n",
    "        assert c_z % self.cfg.pairwise_head_width == 0\n",
    "        block = TriangularSelfAttentionBlock\n",
    "\n",
    "        self.pairwise_positional_embedding = RelativePosition(self.cfg.position_bins, c_z)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                block(\n",
    "                    sequence_state_dim=c_s,\n",
    "                    pairwise_state_dim=c_z,\n",
    "                    sequence_head_width=self.cfg.sequence_head_width,\n",
    "                    pairwise_head_width=self.cfg.pairwise_head_width,\n",
    "                    dropout=self.cfg.dropout,\n",
    "                )\n",
    "                for i in range(self.cfg.num_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.recycle_bins = 15\n",
    "        self.recycle_s_norm = nn.LayerNorm(c_s)\n",
    "        self.recycle_z_norm = nn.LayerNorm(c_z)\n",
    "        self.recycle_disto = nn.Embedding(self.recycle_bins, c_z)\n",
    "        self.recycle_disto.weight[0].detach().zero_()\n",
    "\n",
    "        self.structure_module = StructureModule(**self.cfg.structure_module)  # type: ignore\n",
    "        self.trunk2sm_s = nn.Linear(c_s, self.structure_module.c_s)\n",
    "        self.trunk2sm_z = nn.Linear(c_z, self.structure_module.c_z)\n",
    "\n",
    "        self.chunk_size = self.cfg.chunk_size\n",
    "\n",
    "    def set_chunk_size(self, chunk_size):\n",
    "        # This parameter means the axial attention will be computed\n",
    "        # in a chunked manner. This should make the memory used more or less O(L) instead of O(L^2).\n",
    "        # It's equivalent to running a for loop over chunks of the dimension we're iterative over,\n",
    "        # where the chunk_size is the size of the chunks, so 128 would mean to parse 128-lengthed chunks.\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def forward(self, seq_feats, pair_feats, true_aa, residx, mask, no_recycles: T.Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          seq_feats:     B x L x C            tensor of sequence features\n",
    "          pair_feats:    B x L x L x C        tensor of pair features\n",
    "          residx:        B x L                long tensor giving the position in the sequence\n",
    "          mask:          B x L                boolean tensor indicating valid residues\n",
    "\n",
    "        Output:\n",
    "          predicted_structure: B x L x (num_atoms_per_residue * 3) tensor wrapped in a Coordinates object\n",
    "        \"\"\"\n",
    "\n",
    "        device = seq_feats.device\n",
    "        s_s_0 = seq_feats\n",
    "        s_z_0 = pair_feats\n",
    "\n",
    "        if no_recycles is None:\n",
    "            no_recycles = self.cfg.max_recycles\n",
    "        else:\n",
    "            assert no_recycles >= 0, \"Number of recycles must not be negative.\"\n",
    "            no_recycles += 1  # First 'recycle' is just the standard forward pass through the model.\n",
    "\n",
    "        def trunk_iter(s, z, residx, mask):\n",
    "            z = z + self.pairwise_positional_embedding(residx, mask=mask)\n",
    "\n",
    "            for block in self.blocks:\n",
    "                s, z = block(s, z, mask=mask, residue_index=residx, chunk_size=self.chunk_size)\n",
    "            return s, z\n",
    "\n",
    "        s_s = s_s_0\n",
    "        s_z = s_z_0\n",
    "        recycle_s = torch.zeros_like(s_s)\n",
    "        recycle_z = torch.zeros_like(s_z)\n",
    "        recycle_bins = torch.zeros(*s_z.shape[:-1], device=device, dtype=torch.int64)\n",
    "\n",
    "        assert no_recycles > 0\n",
    "        for recycle_idx in range(no_recycles):\n",
    "            with ExitStack() if recycle_idx == no_recycles - 1 else torch.no_grad():\n",
    "                # === Recycling ===\n",
    "                recycle_s = self.recycle_s_norm(recycle_s.detach())\n",
    "                recycle_z = self.recycle_z_norm(recycle_z.detach())\n",
    "                recycle_z += self.recycle_disto(recycle_bins.detach())\n",
    "\n",
    "                s_s, s_z = trunk_iter(s_s_0 + recycle_s, s_z_0 + recycle_z, residx, mask)\n",
    "\n",
    "                # === Structure module ===\n",
    "                structure = self.structure_module(\n",
    "                    {\"single\": self.trunk2sm_s(s_s), \"pair\": self.trunk2sm_z(s_z)},\n",
    "                    true_aa,\n",
    "                    mask.float(),\n",
    "                )\n",
    "\n",
    "                recycle_s = s_s\n",
    "                recycle_z = s_z\n",
    "                # Distogram needs the N, CA, C coordinates, and bin constants same as alphafold.\n",
    "                recycle_bins = FoldingTrunk.distogram(\n",
    "                    structure[\"positions\"][-1][:, :, :3],\n",
    "                    3.375,\n",
    "                    21.375,\n",
    "                    self.recycle_bins,\n",
    "                )\n",
    "\n",
    "        assert isinstance(structure, dict)  # type: ignore\n",
    "        structure[\"s_s\"] = s_s\n",
    "        structure[\"s_z\"] = s_z\n",
    "\n",
    "        return structure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
