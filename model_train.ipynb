{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "general training procedure   \n",
    "Step 1: Pretraining on Masked Residue  \n",
    "Prepare your protein sequence dataset.  \n",
    "Mask residues randomly (e.g., 15% of tokens) and train the model to predict the   masked residues.  \n",
    "Save the model weights after pretraining.  \n",
    "Step 2: Fine-Tuning on Contact Map Prediction  \n",
    "Prepare your labeled dataset with protein sequences and corresponding contact maps.  \n",
    "Initialize your model with the pretrained weights.  \n",
    "Replace the output layer (if necessary) with one suited for contact map prediction   (e.g., a pairwise scoring mechanism or CNN layers to predict NxN contact maps).  \n",
    "Train on the contact map data in a supervised manner.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.0440\n",
      "Epoch 2/5, Loss: 0.8455\n",
      "Epoch 3/5, Loss: 0.6898\n",
      "Epoch 4/5, Loss: 0.5936\n",
      "Epoch 5/5, Loss: 0.5116\n"
     ]
    }
   ],
   "source": [
    "# example code for training on Masked Residue \n",
    "# generate with ChatGPT\n",
    "# this code has a problem with handlign padding, but the essence is there\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "# Define special tokens\n",
    "MASK_TOKEN = \"<MASK>\"\n",
    "VOCAB = [\"A\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\", \"M\", \"N\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"V\", \"W\", \"Y\"]  # Example vocabulary\n",
    "VOCAB += [MASK_TOKEN]\n",
    "TOKEN_TO_ID = {token: idx for idx, token in enumerate(VOCAB)}\n",
    "ID_TO_TOKEN = {idx: token for token, idx in TOKEN_TO_ID.items()}\n",
    "\n",
    "# Function to generate masked sequences\n",
    "def mask_sequence(sequence, mask_fraction=0.15):\n",
    "    \"\"\"\n",
    "    Mask a fraction of the amino acids in the sequence with the <MASK> token.\n",
    "\n",
    "    Args:\n",
    "        sequence (str): Input amino acid sequence.\n",
    "        mask_fraction (float): Fraction of tokens to mask.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Masked sequence (list of tokens) and target (list of original tokens).\n",
    "    \"\"\"\n",
    "    tokens = list(sequence)\n",
    "    target = tokens.copy()\n",
    "\n",
    "    num_to_mask = int(len(tokens) * mask_fraction)\n",
    "    mask_indices = random.sample(range(len(tokens)), num_to_mask)\n",
    "\n",
    "    for idx in mask_indices:\n",
    "        tokens[idx] = MASK_TOKEN\n",
    "\n",
    "    return tokens, target\n",
    "\n",
    "# Create synthetic dataset\n",
    "def create_dataset(sequences, mask_fraction=0.15):\n",
    "    \"\"\"\n",
    "    Generate masked sequences and targets for training.\n",
    "\n",
    "    Args:\n",
    "        sequences (list): List of amino acid sequences.\n",
    "        mask_fraction (float): Fraction of tokens to mask.\n",
    "\n",
    "    Returns:\n",
    "        list: List of (masked_sequence, target_sequence) pairs.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for seq in sequences:\n",
    "        masked_seq, target_seq = mask_sequence(seq, mask_fraction)\n",
    "        dataset.append((masked_seq, target_seq))\n",
    "    return dataset\n",
    "\n",
    "# Define a simple model\n",
    "class ProteinMLMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=4), num_layers=2\n",
    "        )\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        transformed = self.transformer(embedded)\n",
    "        logits = self.fc(transformed)\n",
    "        return logits\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, dataset, epochs=10, batch_size=8, lr=1e-3):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i in range(0, len(dataset), batch_size):\n",
    "            batch = dataset[i:i+batch_size]\n",
    "            masked_seqs, target_seqs = zip(*batch)\n",
    "\n",
    "            # Convert tokens to IDs\n",
    "            masked_ids = [torch.tensor([TOKEN_TO_ID[tok] for tok in seq]) for seq in masked_seqs]\n",
    "            target_ids = [torch.tensor([TOKEN_TO_ID[tok] for tok in seq]) for seq in target_seqs]\n",
    "\n",
    "            # Pad sequences to the same length\n",
    "            masked_ids = nn.utils.rnn.pad_sequence(masked_ids, batch_first=True, padding_value=TOKEN_TO_ID[MASK_TOKEN])\n",
    "            target_ids = nn.utils.rnn.pad_sequence(target_ids, batch_first=True, padding_value=-100)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(masked_ids)\n",
    "\n",
    "            # Reshape logits and targets for loss computation\n",
    "            logits = logits.view(-1, logits.size(-1))\n",
    "            target_ids = target_ids.view(-1)\n",
    "\n",
    "            loss = criterion(logits, target_ids)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(dataset):.4f}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example sequences\n",
    "    sequences = [\n",
    "        \"ACDEFGHIKLMNPQRSTVWY\",\n",
    "        \"LMNPQRSTVWYACDEFGHI\",\n",
    "        \"QRSTVWYACDEFGHIKLMN\",\n",
    "    ]\n",
    "\n",
    "    # Generate dataset\n",
    "    dataset = create_dataset(sequences, mask_fraction=0.15)\n",
    "\n",
    "    # Initialize model\n",
    "    model = ProteinMLMModel(vocab_size=len(VOCAB), embedding_dim=32, hidden_dim=64)\n",
    "\n",
    "    # Train model\n",
    "    train_model(model, dataset, epochs=5)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
